\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\usepackage{mathrsfs}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}
\subsection*{1}
\ssn{a}
The value to be minimized is the negative of the entropy, or
\[\sum_{i=1}^K p_i\log p_i\]
Our equality constraints are those imposed by the expectations and that the probabilities must sum to $1$. Define $T_0(x)=1$ for all $x$ and $\alpha_0=1$, so then we can write for $r=0..R$, we have the constraint 
\[\alpha_r=\sum_{i=1}^KT_r(i)p_i\]
which includes the sum constraint. Writing the $T$s as a vector in $\rn^K$, we can express this in the form $\alpha_r-p^TT_r=0$.

Finally, we have an inequality constraint, which is that all probabilities must be nonnegative, or $p^Te_i\geq0$ for all $i=1..K$, where $e_i$ are the standard basis vectors
\ssn{b}
The Lagrangian for the system above is 
\[\sum_{i=1}^K p_i\log p_i-\sum_{r=0}^R\lambda_r(\alpha_r-p^TT_r)-\sum_{i=1}^K\mu_ip^Te_i\]
Its gradient wrt $p$ is 
\[e+\log p+\sum_{r=0}^R\lambda_rT_r-\sum_{i=1}^K\mu_ie_i\]
where $e$ is the vector of all $1$s and the log is taken componentwise. Setting this equal to $0$ gives
\[\log p=-\sum_{r=0}^R\lambda_rT_r+\mu-e\]
Componentwise, we have $p_i=\exp\left(\mu_i-\sum_{r=0}^R\lambda_r T_r(i)-1\right)$. If we let $u(i)=\openm e_i\\Te_i\closem$ ($T$ being the matrix of values of the $T_r$), then we can write
\[p(i)=e^{-1}\exp\left(\openm \mu^T &\lambda^T\closem u(i)\right)\]
which is in the form of an exponential family, with the Lagrange multiplies $\mu_i$ and $\lambda_i$ being the parameters.
\ssn{c}
If we look at $T$ as data rather than imposed constraints, then we can view $u$ as a function of $T$. The moment constraints $\alpha_i$ then become sufficient statistics, and we can compute the Lagrange multipliers as a ML problem rather than a constrained optimization problem.
\subsection*{2}
\ssn{a}
The joint distribution of $Y$ and $Z$ is bivariate normal with mean $\openm\mu\\0\closem$ and covariance $\openm \sigma^2&0\\0&\sigma_1^2\closem$. We can obtain the joint distribution of $X=Y+Z$ and $Y$ from this by transforming via the matrix $\openm 1&0\\1&1\closem$, which results in a bivariate normal with mean $\openm\mu\\\mu\closem$ and covariance $\openm\sigma^2&\sigma^2\\\sigma^2&\sigma^2+\sigma_1^2\closem$ (this is arranged as $\openm Y\\X\closem$).
\ssn{b}
If we condition on $X=x$, then the density is univariate normal, with mean 
\[\mu+\frac{\sigma}{\sqrt{\sigma^2+\sigma_1^2}}\frac{\sigma^2}{\sigma\sqrt{\sigma^2+\sigma_1^2}}(x-\mu)=\mu+\frac{\sigma^2}{\sigma^2+\sigma_1^2}(x-\mu)\]
and variance $\left(1-\frac{\sigma^2}{\sigma^2+\sigma_1^2}\right)\sigma^2$
\ssn{c}
The marginal distribution of $X$ is normal with mean $\mu$ and variance $\sigma^2+\sigma_1^2$. Then, the ML estimate for $\mu$ is the sample mean of the observed $X^{(n)}$, and the ML estimate for $\sigma^2$ is the sample covariance minus $\sigma_1^2$.
\ssn{d}
First, we can write the joint density of $X$ and $Y$ as $f(X|Y=y)f(y)$. Since we have the identity $X=Y+Z$, $X|Y=y$ is normal with mean $y$ and variance $\sigma_1^2$. Thus, the log-likelihood of the complete data is 
\[\sum_{i=1}^N\log f(x_i|Y=y_i)+\log f(y_i)=\sum_{i=1}^N-\log(2\pi)-\log\sigma_1-\log\sigma'-\frac{(x_i-y_i)^2}{2\sigma_1^2}-\frac{(y_i-\mu')^2}{2\sigma'^2}\]
where the $\mu'$ and $\sigma'$ are yet-to-be determined. We can discard the constant terms then take the expectation wrt to $Y$ given $X$ to obtain
\[-\sum_{i=1}^N\int_\rn p(y|x_i,\mu,\sigma^2)\left(\frac{(x_i-y)^2}{2\sigma_1^2}+\frac{(y-\mu')^2}{2\sigma'^2}+\log\sigma'\right)\ dy\]
$p(y|x_i, \mu, \sigma^2)$ is a quantity calculated from the parameters of the previous iteration. As derived in (b), it is the normal density with the appropriate mean and variance (written above, not going to rewrite here). Abbreviate this function as $q_i(y)$ from now on.

To find the next value of $\mu$, we differentiate wrt $\mu'$ and set to 0 to obtain
\begin{align*}
    0&=\sum_{i=1}^N\int_\rn \frac{1}{\sigma'^2}q_i(y)(y-\mu')\ dy\\
    \mu'\sum_{i=1}^N\int_{\rn}q_i(y)\ dy&=\sum_{i=1}^N\int_\rn yq_i(y)\ dy\\
    \mu'&=\frac{1}{N}\sum_{i=1}^NE(Y|x_i,\mu,\sigma^2)\\
        &=\mu+\frac{1}{N}\frac{\sigma^2}{\sigma^2+\sigma_1^2}\sum_{i=1}^N(x_i-\mu)\\
\end{align*}

Now, differentiating wrt $\sigma'$ and setting to $0$ gives
\begin{align*}
    0&=\sum_{i=1}^N\int_\rn q_i(y)\left(\frac{(y-\mu')^2}{\sigma'^3}-\frac{1}{\sigma'}\right)\,dy\\
     &=\sum_{i=1}^N\int_\rn q_i(y)\left(\frac{(y-\mu')^2}{\sigma'^2}-1\right)\,dy\\
    N&=\frac{1}{\sigma'^2}\sum_{i=1}^N\int_\rn q_i(y)(y-\mu')^2\,dy\\
    \sigma'^2&=\frac{1}{N}\sum_{i=1}^NE_{Y|X=x_i}(Y-\mu')^2\\
\end{align*}

The expectation inside the sum expands to $\var(Y)+E(Y)^2-2\mu' E(Y)+\mu'^2$, so the final expression for $\sigma'^2$ is
\[\mu'^2+\frac{1}{N}\sum_{i=1}^N\var(Y|x_i)+E(Y|x_i)^2-2\mu'E(Y|x_i)\]
where the dependence on the previous $\sigma$ comes in from the conditional expectations and variance
\ssn{e}
If we let $\mu$ be the sample mean of the $X_i$ in the expression for $\mu'$ above, then the sum cancels out to zero and we are left with $\mu'=\mu$, which means that it's a fixed point. This is the only fixed point, since the equation obtained by setting $\mu'=\mu$ is linear, which means that there's only one solution.
\ssn{f}
If we set $\mu=\mu'=\h{\mu}_{ML}$, then our expression for $\sigma'^2$ becomes
\[\h{\mu}^2+\left(1-\frac{\sigma^2}{\sigma^2+\sigma_1^2}\right)\sigma^2+\frac{1}{N}\sum_{i=1}^N\left(\h{\mu}+\frac{\sigma^2}{\sigma^2+\sigma_1^2}(x_i-\h{\mu})\right)^2-\frac{2\h{\mu}}{N}\sum_{i=1}^N\h{\mu}+\frac{\sigma^2}{\sigma^2+\sigma_1^2}(x_i-\h{\mu})\]
The term with the unadorned sum of the $x_i-\h{\mu}$ sums out to zero, so canceling that and expanding the square leaves us with
\[\left(1-\frac{\sigma^2}{\sigma^2+\sigma_1^2}\right)\sigma^2+\frac{1}{N}\sum_{i=1}^N2\h{\mu}\frac{\sigma^2}{\sigma^2+\sigma_1^2}(x_i-\h{\mu})+\frac{1}{N}\sum_{i=1}^N\left(\frac{\sigma^2}{\sigma^2+\sigma_1^2}(x_i-\h{\mu})\right)^2\]
The middle term again disappears, and if we're assuming that $\sigma=\h{\sigma}_{ML}$, then we can rewrite the last term:
\[\sigma^2-\frac{\sigma^4}{\sigma^2+\sigma_1^2}+\frac{\sigma^4(\sigma^2+\sigma_1^2)}{(\sigma^2+\sigma_1^2)^2}=\sigma^2\]
so we have that the ML estimate is a fixed point.
\subsection*{3}
\ssn{a}
Let $Y_1,\ldots, Y_N$ be the hidden class variables. The complete-data log-likelihood is then
\[\sum_{i=1}^N\left(\log \pi_{y_i}+\sum_{j=1}^dx_{i,j}\log p_{j,y_i}+(1-x_{i,j})\log(1-p_{j, y_i})\right)\]
Taking the expectation wrt $Y$ given previous parameters and the data yields
\[\sum_{i=1}^N\sum_{m=1}^Mw_{i,m}\left(\log\pi_m+\sum_{j=1}^dx_{i,j}\log p_{j,m}+(1-x_{i,j})\log(1-p_{j,m})\right)\]
The ML estimate of the $\pi_m$ is, as usual, $\frac{1}{N}\sum_{i=1}^Nw_{i,m}$. Differentiating wrt each $p_{j,m}$, we have
\[\sum_{i=1}^Nw_{i,m}\left(\frac{x_{i,j}}{p_{j,m}}-\frac{1-x_{i,j}}{1-p_{j,m}}\right)\]
Setting this equal to zero produces 
\[p_{j,m}=\frac{\sum_{i=1}^Nw_{i,m}x_{i, j}}{\sum_{i=1}^N w_{i,m}}\]
\ssn{b}
We want conjugate priors for $\pi$ and $p$. Since the conjugate priors for the Bernoulli and the multinomial are the beta and the Dirichlet, respectively, we use $Md$ independent beta densities for the $p_{j,m}$ and a Dirichlet for all the $\pi$s. For simplicity's sake, initialize all the beta distributions with the same hyperparameter $\alpha$. Let the initial hyperparameters for the Dirichlet be $\zeta_m$.

The quantity to maximize is now $l(\theta|X,Y)+\log p(\theta)$, which in this case is
\begin{gather*}
    \sum_{i=1}^N\sum_{m=1}^Mw_{i,m}\left(\log\pi_m+\sum_{j=1}^dx_{i,j}\log p_{j,m}+(1-x_{i,j})\log(1-p_{j,m})\right)\\
    +\sum_{i=1}^N\sum_{m=1}^M\sum_{j=1}^d\frac{1}{N}\left((\alpha-1)\log p_{j,m}+(\alpha-1)\log(1-p_{j,m})\right)\\
    +\frac{1}{N}\sum_{i=1}^N\sum_{m=1}^M(\zeta_m-1)\log\pi_m
\end{gather*}
after removing constants irrelevant to maximization. Folding the sums in gives
\[\sum_{i=1}^N\sum_{m=1}^M\left(w_{i,m}+\frac{1}{N}(\zeta_m-1)\right)\log\pi_m+\sum_{i=1}^N\sum_{m=1}^Mw_{i,m}\sum_{j=1}^d\left(x_{i,j}+\frac{1}{N}(\alpha-1)\right)\log p_{j,m}+\left(1-x_{i,j}+\frac{1}{N}(\alpha-1)\right)\log(1-p_{j,m})\]
Having the same form as the thing we maximized already, we find that the new equations for $\pi_m$ and $p_{j,m}$ are
\begin{align*}
    \pi_m&=\kappa\sum_{i=1}^N\left(w_{i,m}+\frac{1}{N}(\zeta_m-1)\right)\\
    p_{j,m}&=\frac{\sum_{i=1}^Nw_{i,m}\left(x_{i,j}+\frac{1}{N}(\alpha-1)\right)}{\left(\frac{2}{N}(\alpha-1)+1\right)\sum_{i=1}^Nw_{i,m}}
\end{align*}
where $\kappa$ is some constant that makes the $\pi_m$ sum to 1.

For computational purposes in computing the $w_{i,m}$, instead of computing
\[\frac{P(x_i|Y=m)P(Y=m)}{\sum_{j=1}^MP(x_i|Y=j)P(Y=j)}\]
compute instead
\[\log (P(x_i|Y=m)P(Y=m))=\sum_{j=1}^d\left(x_{i,j}\log p_{j,m}+(1-x_{i,j})\log(1-p_{j,m})\right)+\log\pi_m\]
for each pair $i,m$ and call them the $\alpha_{i,m}$. For each $i$, calculate $l_i=\max_m\alpha_{i,m}$, and set $w_{i,m}=\frac{\exp(\alpha_{i,m}-l_i)}{\sum_{m'}\exp(\alpha_{i,m'}-l_i)}$.
\end{document}
