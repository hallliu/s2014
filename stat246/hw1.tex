\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}
\subsection*{1}
Over such a thin shell of thickness $\ep$, we can assume the density function to be roughly constant. Then, evaluating the density function at any point $x$ with norm $r$ gives
\[\frac{1}{(2\pi\sigma^2)^{d/2}}\exp\left(\frac{-1}{2}x^T\sigma^{-2}Ix\right)=\frac{1}{(2\pi\sigma^2)^{d/2}}\exp\left(\frac{-r^2}{2\sigma^2}\right)\]
The hypervolume of the surface of the sphere is $S_dr^{d-1}$ (since it's a $d-1$-dimensional set), so the volume of the shell is approximately $\ep S_dr^{d-1}$. The integral of the density over the shell is approximately the volume of the shell times the value of the density, or 
\[\ep\frac{S_dr^{d-1}}{(2\pi\sigma^2)^{d/2}}\exp\left(\frac{-r^2}{2\sigma^2}\right)\]
To find the maximum of $f$, differentiate it by $r$ to obtain
\[\frac{(d-1)S_dr^{d-2}}{(2\pi\sigma^2)^{d/2}}\exp\left(\frac{-r^2}{2\sigma^2}\right)-\frac{S_dr^d}{\sigma^2(2\pi\sigma^2)^{d/2}}\exp\left(\frac{-r^2}{2\sigma^2}\right)\]
Setting this equal to $0$ and canceling gives
\[0=(d-1)-\frac{r^2}{\sigma^2}\implies r=\pm\sigma\sqrt{d-1}\]
Now, to check that this is a maximum, taking the second derivative gives us a common, positive multiplier times the following:
\[(d-1)(d-2)-\frac{r^2}{\sigma^2}(2d-1)+\frac{r^4}{\sigma^4}\]
Plugging in the previously obtained value for the stationary point, we have
\[d^2-3d+2-(d-1)(2d-1)+(d-1)^2=-2d+2\]
which becomes negative for large $d$. Thus, the point is a maximum, and we can estimate it by $\sigma\sqrt{d}$ for large $d$.

Plugging $r^*+\ep$ to $f$, we get 
\[\frac{S_d(r^*+\ep)^{d-1}}{(2\pi\sigma^2)^{d/2}}\exp\left(-\frac{{r^*}^2+2r^*\ep+\ep^2}{2\sigma^2}\right)\]
Ignoring the first-order term in $\ep$ inside the exponential, we're left with
\[\frac{S_d(r^*+\ep)^{d-1}}{(2\pi\sigma^2)^{d/2}}\exp\left(-\frac{{r^*}^2}{2\sigma^2}\right)\exp\left(-\frac{\ep^2}{2\sigma^2}\right)\]
As $\ep$ grows larger, the decay from the exponential term will dominate the at-most-order-$d-1$ contribution from $(r^*+\ep)^{d-1}$, so we can approximate this as 
\[f(r^*)\exp\left(-\frac{\ep^2}{2\sigma^2}\right)\]
Thus, the value of $f$ decays as the exponential of the square of the distance from the maximum point, which means that the mass of the density in shells away from that optimal radius is in fact very low.
\subsection{2}
By the AM-GM inequality, we have $\frac{a+b}{2}\leq\sqrt{ab}$. Since the minimum of $a$ and $b$ is bounded above by the arithmetic mean, we thus also have $\min(a,b)\leq\sqrt{ab}$. 

Now, we have that $P(\text{error})=P(\text{error}|Y=1)P(Y=1)+P(\text{error}|Y=2)P(Y=2)$. If we let $\Sigma_i$ be the set which the classifier classifies as $\h{Y}=i$, then the above is equal to 
\[\int_{\Sigma_2}P(x|Y=1)P(Y=1)dx+\int_{\Sigma_1}P(x|Y=2)P(Y=2)dx\]
Over $\Sigma_1$, we have that $P(x|Y=1)P(Y=1)\leq P(x|Y=2)P(Y=2)$ by the form of the Bayes classifier, and vice versa for $\Sigma_2$. Thus, the integrals above are bounded above by
\begin{align*}
&\frac{1}{2}\int_{\Sigma_2}P(x|Y=1)P(Y=1)+P(x|Y=2)P(Y=2)dx+ \frac{1}{2}\int_{\Sigma_1}P(x|Y=1)P(Y=1)+P(x|Y=2)P(Y=2)dx\\
    &=\frac{1}{2}\int_{\rn^n}P(x|Y=1)P(Y=1)+P(x|Y=2)P(Y=2)dx
\end{align*}
if we replace the integrands with the average of the two integrands. Now, using the AM-GM inequality, we have that this is bounded above by 
\begin{align*}
\int_{\rn^n}\sqrt{P(x|Y=1)P(Y=1)P(x|Y=2)P(Y=2)}dx&=\sqrt{P(Y=1)P(Y=2)}\int_{\rn^n}\sqrt{P(x|Y=1)P(x|Y=2)}dx\\
&\leq\frac{1}{2}\int_{\rn^n}\sqrt{P(x|Y=1)P(x|Y=2)}dx
\end{align*}
by another application of AM-GM to $\sqrt{P(Y=1)P(Y=2)}$, noting that the average of $P(Y=1)$ and $P(Y=2)$ is $\frac{1}{2}$.
\subsection*{3}
\ssn{a}
The decision boundary is the point at which $P(x|Y=1)P(Y=1)=P(x|Y=2)P(Y=2)$, or $\pi_1\exp\left((x-\mu_1)^2/(2\sigma^2)\right)=\pi_2\exp\left((x-\mu_2)^2/(2\sigma^2)\right)$. Solving for $x$, we have 
\[x^*=\frac{2\sigma\log\frac{\pi_1}{\pi_2}+\mu_2^2-\mu_1^2}{2(\mu_2-\mu_1)}\]
\ssn{b}
The error probability is
\[\int_{-\infty}^{x^*}P(x|Y=2)P(Y=2)dx+\int_{x^*}^\infty P(x|Y=1)P(Y=1)dx=\pi_2\Phi\left(\frac{x^*-\mu_2}{\sigma}\right)+\pi_1-\pi_1\Phi\left(\frac{x^*-\mu_1}{\sigma}\right)\]

Rewrite $x^*-\mu_2=\frac{2\sigma\log\frac{\pi_1}{\pi_2}}{2(\mu_2-\mu_1)}+\frac{\mu_2+\mu_1}{2}-\mu_2$. Then, dividing this by $\sigma$ produces a term constant in $\sigma$ and a term that goes to $-\infty$ as $\sigma\to0$. Thus, the $\pi_2$ term in the error vanishes. Similarly, $\frac{x^*-\mu_1}{\sigma}$ goes to $\infty$ as $\sigma\to0$, which means that the overall error term approaches $\pi_1-\pi_1=0$.
\ssn{c}
As $\pi_1\to0$, the log term in the decision boundary approaches $-\infty$, dragging the decision boundary with it, as all other things are fixed. In this case, always classifying things as class 2 would produce an error rate of $\pi_1$, which is known to be small.
\ssn{d}
{
\nc{\lot}{L_{1,2}}
\nc{\lto}{L_{2,1}}
Let $S_1$ be the set of points in class $1$ and $S_2$ the same for class 2. If we predict class 1, the expected loss is $E_X(\lto P(Y=2|X)$. If we predict class 2, then it is $E_X(\lot P(Y=1|X))$. If we minimize this pointwise, we should predict class 1 when $\lto P(Y=2|X)\leq\lot P(Y=1|X)$, and class 2 otherwise.

Rewriting the conditional probabilities, the prediction is then $\text{argmax}\left(\lto P(X|Y=2)P(Y=2), \lot P(X|Y=1)P(Y=1)\right)$. Setting these equal to find the boundary, we have
\[\lto \pi_2\exp\left((x-\mu_2)^2/(2\sigma^2)\right)=\lot\pi_1\exp\left((x-\mu_1)^2/(2\sigma^2)\right)\]
Solving for $x$, we obtain
\[x^*=\frac{2\sigma\log\frac{\lot\pi_1}{\lto\pi_2}+\mu_2^2-\mu_1^2}{2(\mu_2-\mu_1)}\]
Now, if we choose $\lot$ to be proportional to $\pi_1^{-1}$ and choose $\lto$ to be proportional to $\pi_2^{-1}$ (possibly with different constants), we will avoid the degeneracy problem.
\ssn{e}
Using these values, we have that $x^*=1$, so plugging into the expression for the error rate from (b) gives $0.5\Phi(-1)+0.5-0.5\Phi(1)=0.1587$
}
\subsection*{4}
The expected loss of some decision function $\h{H}$ is $E_{X, Y}(L(\h{H}(x), Y)$. Writing out the integral gives
\[\int_{\rn^n}\sum_{k=1}^KL(\h{H}(x), k)p(x, k) dx=\int_{\rn^n}p(x)\sum_{k=1}^KL(\h{H}(x), k)P(k|x)dx\]

%If we define $R_i$ as the subset of $\rn^n$ for which $\h{H}(x)=i$, we can separate the integral as follows
%\[\sum_{i=1}^K\int_{R_i}p(x)\sum_{k=1}^KL(i, k)P(k|x)dx\]
%If there is some subset $A$ of $R_i$ for which there exists a $j$ such that  for all $x\in A$, then moving $A$ from $R_i$ to $R_j$ will reduce the value of the expected loss by comparison 

Define $R_i$ as $\{x\in\rn^n|\h{H}(x)=i\}$. Then, suppose that there exists some subset of nonzero measure $A$ of $R_i$ such that $\sum_{k=1}^KL(i, k)P(k|x)\ge\sum_{k=1}^KL(j, k)P(k|x)$. Decompose the above integral as
\[\int_{\rn^n-A}p(x)\sum_{k=1}^KL(\h{H}(x), k)P(k|x)dx+\int_Ap(x)\sum_{k=1}^KL(i, k)P(k|x)dx\]
Then, changing the class of $A$ from $i$ to $j$ will result in a strict decrease in the expected loss, which means that the optimal decision rule must satisfy, for each $i$, $\sum_{k=1}^KL(i, k)P(k|x)\ge\sum_{k=1}^KL(j, k)P(k|x)$ for all $j\neq i$ and almost all $x\in R_i$. The rule given satisfies this condition, and since there are a finite number of classes, changing the rule on a set of measure zero from the region associated with each class will not result in a change in the expected loss. Thus, the rule given is optimal.
\subsection*{5}
\ssn{a}
We have $\cov(X_2,X_2)=v_2$, so writing $X_2=\alpha X_1+Z$ gives
\[\cov(\alpha X_1+Z, \alpha X_1+Z)=\cov(\alpha X_1, \alpha X_1)+0+\cov(Z, Z)=\alpha^2v_1+\var(Z)=v_2\]
In addition, using $\cov(X_1, X_2)=a$ gives us
\[a=\cov(X_1, \alpha X_1+Z)=\alpha v_1+0\implies \alpha=\frac{a}{v_1}\]
Substituting into the first equation gives us $\var(Z)=v_2-\frac{a^2}{v_1}$

Finally, since $E(X_1)=E(X_2)=0$, we must also have $E(Z)=0$.
\ssn{b}
We know that $Z$ is Gaussian because we can express $Z$ as the sum of two Gaussians, $X_2-\alpha X_1$. Thus, it is independent from $X_1$ because their covariance is zero. The variance was derived above.
\ssn{c}
We have $E(X_2|X_1=x)=E(\alpha x+Z)=\alpha x$ and $\var(X_2|X_1=x)=\var(\alpha x+Z)=\var(Z)$. 
\ssn{d}
Since the samples are all independent, the cross terms in the formula for the variance of a sum disappear and we get
\[\var(\h{a})=\frac{1}{n^2}\sum_{i=1}^n\var(X_{i1}X_{i2})=\frac{1}{n}\var(X_1X_2)\]
Then, we have $\var(X_1X_2)=E((X_1X_2)^2)-E(X_1X_2)^2=E((X_1X_2)^2)-a^2$, since the means are zero.

%To obtain the first term, note that it is a cross moment and can be obtained by applying $\frac{\partial^4}{\partial x_1^2\partial x_2^2}$ to the mgf of the distribution and evaluating at zero. The mgf itself is
%\[\exp\left(\frac{1}{2}\openm x_1&x_2\closem\openm v_1&a\\a&v_2\closem\openm x_1\\x_2\closem\right)=\exp\left(\frac{1}{2}(x_1^2v_1+2x_1x_2a+x_2^2v_2)\right)\]
%
%Differentiating this wrt $x_1$ twice and wrt $x_2$ once gives 
%\[ax_1v_1T+x_1v_1(x_1v_1+x_2a)(x_2v_2+x_1a)T+v_1(x_2v_2+x_1a)T\]
%where $T$ is the exponential term from above. Then, taking the derivatives and then evaluating gives $v_1v_2$, so the variance of $\h{a}$ is $\frac{1}{n}(v_1v_2-a^2)$.
Write $X_1^2X_2^2=X_1^2(\alpha X_1+Z)^2$. Then, the expectation of this is
\[\alpha^2E(X_1^4)+2\alpha E(Z)E(X_1^3)+E(X_1^2)E(Z^2)=\alpha^2E(X_1^4)+E(X_1^2)E(Z^2)\]
By differentiating the mgf for $X_1$ four times and evaluating at zero, we obtain $E(X_1^4)=3v_1$. We have that $E(X_1^2)=v_1$ and $E(Z^2)=\var(Z)$, so the resulting expression for $E(X_1^2X_2^2)$ is
\[3\frac{a^2}{v_1^2}v_1^2+v_1\left(v_2-\frac{a^2}{v_1}\right)=v_1v_2+2a^2\]
which gives us $\var(\h{a})=\frac{1}{n}(v_1v_2+a^2)$
\subsection*{6}
\ssn{a}
$\Sigma$ is positive definite because $A$ is nonsingular -- computing $x^T\Sigma x=x^TA^TAx=\|Ax\|_2^2$ will result in a positive value as long as $x$ is nonzero. The eigenvalues of $\Sigma$ are the squared eigenvalues of $A$, so the lowest eigenvalue of $\Sigma$ is $0.01$.
\ssn{b}
Sampling from a multivariate normal distribution with some specified covariance matrix can be done by first obtaining an iid sample of $n$ standard normal random variables, then transforming it via left-multiplication by $A^T$. Python code is attached.
\ssn{c,d}
Histograms for $e$: $d=100$ on the right, $d=10$ on the left. $n=500$ on top, $n=5000$ on bottom.

\includegraphics[width=0.5\textwidth]{hw1_files/e10-500.png}
\includegraphics[width=0.5\textwidth]{hw1_files/e100-500.png}

\includegraphics[width=0.5\textwidth]{hw1_files/e10-5000.png}
\includegraphics[width=0.5\textwidth]{hw1_files/e100-5000.png}

The mean and stddev for $e$ are in the following table (presented as a pair mean,stddev):

\begin{tabular}{c|c|c}
    &$d=10$&$d=100$\\
    \hline
    $n=500$&$0.447,64.85$&$0.0302,60.67$\\
    \hline
    $n=5000$&$-4.137,204.75$&$0.00706,200.00$\\
\end{tabular}

Histograms for the eigenvalue estimates: $d=100$ on right, $d=10$ on left

\includegraphics[width=0.5\textwidth]{hw1_files/m10-500.png}
\includegraphics[width=0.5\textwidth]{hw1_files/m100-500.png}

\includegraphics[width=0.5\textwidth]{hw1_files/m10-5000.png}
\includegraphics[width=0.5\textwidth]{hw1_files/m100-5000.png}

The table of means and standard deviations:

\begin{tabular}{c|c|c}
    &$d=10$&$d=100$\\
    \hline
    $n=500$&$-0.0214,0.0626$&$-0.274,0.0669$\\
    \hline
    $n=5000$&$-0.00140,0.0205$&$-0.0239,0.0197$\\
\end{tabular}

As expected, the standardized errors cluster around zero with an apparently normal distribution. The standard deviation is lower for $d=100$ than $d=10$ and higher for $n=5000$ than $n=500$. The eigenvalue ratios indicate that the highest eigenvalue of the inverse of the sample covariance is consistently lower than that of the highest eigenvalue of the actual.
\end{document}
