\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\DeclareMathOperator{\flatv}{flat}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}
\subsection*{1}
The mgf of $Y$ is $M_Y(t)=E(e^{t^TY})=E(e^{t^TAX})=E(e^{(A^Tt)^TX})=M_X(A^Tt)$, which we can write as $\exp\left(t^TA\mu+\frac{1}{2}t^TA\Sigma A^Tt\right)$. This is the mgf of a normal RV with mean $A\mu$ and covariance $A\Sigma A^T$, so $Y$ must be also distributed as such.

\subsection*{2}
\ssn{a}
We have that $f(x,y)=f_Y(y|x)f_X(x)$, so $f(x,y)$ is jointly normal iff this product has the form of one. In (b), we show that there exists a particular definition for $Z$ such that $f_Z(z|x)=f_Y(y|x)$ and $X$ and $Z$ are jointly normal. Thus, since $f(x,z)$ and $f(x,y)$ are equivalent, $f(x,y)$ is normal.
\ssn{b}
Let $Z=AX+b+U$, $U\sim N(0,Q)$ and independent of $X$. Then, if given a value for $X$, we have $Z=Ax+b+U$, so consequently we have $f(z|x)=N(Ax+b, Q)$, the same as that of $f(y|x)$.

The joint mgf of $X$ and $Z$ can be written as 
\[E\left(\exp\left(\openm t_1^T&t_2^T\closem\openm X\\Z\closem\right)\right)=E(\exp(t_1^TX+t_2^T(AX+b+U)))=E(\exp((t_1+A^Tt_2)^TX)\exp(t_2^Tb)\exp(t_2^TU))\]
Resolving the expected value and expressing the result using the mgfs of $X$ and $U$, we have
\[\exp\left((t_1+A^Tt_2)^T\mu+t_2^Tb+\frac{1}{2}(t_1+A^Tt_2)^T\Sigma (t_1+A^Tt_2)+\frac{1}{2}t_2^TQt_2\right)\]
Now, we note that 
\[(t_1+A^Tt_2)^T\mu+t_2^Tb=\openm t_1^T&t_2^T\closem\openm\mu\\A\mu+b\closem\text{ and}\]
\[(t_1+A^Tt_2)^T\Sigma (t_1+A^Tt_2)+t_2^TQt_2=\openm t_1^T&t_2^T\closem\openm \Sigma &\Sigma A^T\\A\Sigma &A\Sigma A^T+Q\closem\openm t_1\\t_2\closem\]
Thus, if we let $t=\openm t_1\\t_2\closem$, $\mu'=\openm\mu\\A\mu+b\closem$, and $\Sigma'=\openm \Sigma &\Sigma A^T\\A\Sigma &A\Sigma A^T+Q\closem$, we have that the joint mgf of $X,Z$ is $\exp\left(t^T\mu'+\frac{1}{2}t^T\Sigma't\right)$, which means that it's multivariate normal with mean $\mu'$ and covariance $\Sigma'$.
\ssn{c}
The marginal is simply restricting the joint distribution to the relevant components. In the case of $Y$, its marginal distribution is then $N(A\mu+b, A\Sigma A^T+Q)$. For the density of $X$ conditional on $Y=y$, (2.81) and (2.82) in the book give us that it's normal with $\mu_{x|y}=\mu+\Sigma A^T(A\Sigma A^T+Q)^{-1}(y-A\mu+b)$ and $\Sigma_{x|y}=\Sigma-\Sigma A^T(A\Sigma A^T+Q)^{-1}A\Sigma$
\subsection*{3}
The likelihood of the data is 
\[\prod_{i=1}^N\frac{C}{\sqrt{|\Sigma|}}\exp\left(-\frac{1}{2}(X_i-\mu_{k_i})^T\Sigma^{-1}(X_i-\mu_{k_i})\right)\pi_{k_i}\]
where $C$ is a constant that depends on none of the parameters. We have immediately that $\h{\pi}_{k_i}$ is $\frac{\#i}{N}$, where $\#i$ is the number of occurrances of class $i$ in the data. Taking the loglikelihood (and ignoring the constant $C$ and the factor of $\frac{1}{2}$), we have
\[\sum_{i=1}^N-\log|\Sigma|+(X_i-\mu_{k_i})^T\Sigma^{-1}(X_i-\mu_{k_i})+\log(\pi_{k_i})\]
To minimize this expression over the $\mu_k$, write the expression with an inner sum over points with the same class:
\[-N\log|\Sigma|+\sum_{k=1}^K\sum_{X:Y(X)=k}(X-\mu_k)^T\Sigma^{-1}(X-\mu_k)\]
The $\pi_{k_i}$ terms have been discarded because they are not dependent on the parameters. Now, we can simply minimize over each class, and find that $\mu_k$ is the sample mean over all points that are in class $k$. 

Now, let $S_k$ be the sample covariance over class $k$, or $\sum_{i=1}^{\#k}(X_i-\conj{X})(X_i-\conj{X})^T$. Then, we can write the log-likelihood (divided out by $N$) as
\[-\log|\Sigma|+\frac{1}{N}\sum_{i=1}^N\tr\left((X_i-\mu_{k_i})(X_i-\mu_{k_i})^T\Sigma^{-1}\right)=-\log|\Sigma|+\frac{1}{N}\sum_{k=1}^K\tr\left(\#kS_k\Sigma^{-1}\right)=-\log|\Sigma|+\tr\left(\left(\sum_{k=1}^K\frac{\#k}{N}S_k\right)\Sigma^{-1}\right)\]
It was shown that minimizing an expression of the form $-\log|\Sigma|+\tr(A\Sigma^{-1})$ over $\Sigma$ is given by $\Sigma=A$, so we have that the MLE for the pooled covariance is $\sum_{k=1}^K\frac{\#k}{N}S_k$
\subsection*{4}
Let $g(\eta)=\exp(A(\eta))$. Then, we have $f(x;\eta)=\frac{h(x)}{g(\eta)}e^{\eta^TT(x)}$, and integrating this over $\rn^n$ (the space $x$ lies in) gives $1$, so we have $g(\eta)=\int_{\rn^n}h(x)e^{\eta^TT(x)}dx$.

Taking the first derivative of $g$ gives $\nabla g(\eta)=e^{A(\eta)}\nabla A(\eta)$ and 
\[\nabla g(\eta)=\int h(x)e^{\eta^TT(x)}T(x)dx\implies\nabla A(\eta)=\int h(x)e^{\eta^TT(x)-A(\eta)}T(x)=E(T(x))\]
Using the notation $\nabla^2 g$ for the second derivative, we have that $\nabla^2g(\eta)=e^{A(\eta)}\left(\nabla^2A(\eta)+\nabla A(\eta)\nabla A(\eta)^T\right)$ and
\[\nabla^2 g(\eta)=\int h(x)e^{\eta^TT(x)}T(x)T(x)^Tdx\]
Combining the two expressions gives us 
\[e^{A(\eta)}\left(\nabla^2 A(\eta)+E(T(x))E(T(x)^T)\right)=\int h(x)e^{\eta^TT(x)}T(x)T(x)^Tdx\]
Dividing out by $e^{A(\eta)}$ and note that the RHS is equal to $E(T(x)T(x)^T)$, so subtracting off $E(T(x))E(T(x)^T)$ from both sides gives $\nabla^2A(\eta)=E(T(x)T(x)^T)-E(T(x))E(T(x)^T)$, which is the covariance of $T(x)$.
\subsection*{5}
The density of the multivariate normal distribution can be written as 
\[\frac{1}{(2\pi)^{d/2}|\Sigma|}\exp\left(-\frac{1}{2}\tr\left((x-\mu)(x-\mu)^T\Sigma^{-1}\right)\right)\]
We can let $(2\pi)^{-d/2}$ be $h(x)$ and put $|\Sigma^{-1}|$ into $g(\mu, \Sigma)$, so we are left with considering the exponential term. Expanding out the exponent, we get
\[\frac{-1}{2}\tr\left(xx^T-\mu x^T-x\mu^T+\mu\mu^T)\Sigma^{-1}\right)=-\frac{1}{2}\left(\tr(xx^T\Sigma^{-1})-2\tr(x\mu^T\Sigma^{-1})+\tr(\mu\mu^T\Sigma^{-1})\right)\]
Now, let $g(\mu,\Sigma)=|\Sigma^{-1}|\exp\left(-\frac{1}{2}\mu^T\Sigma^{-1}\mu\right)$, so we can now discard the last term from the above expression and try to find what $T(x)$ is. We're left with
\[-\frac{1}{2}\tr(xx^T\Sigma^{-1})+\mu^T\Sigma^{-1}x\]

Define $\flatv(A)$ for a $n\times m$ matrix $A$ to be a $nm$-long column vector that consists of the rows of $A$ laid out in sequential order. Then, we have that $\tr(AB)=\flatv(A)^T\flatv(B^T)$ for matrices $A,B$. Let $\eta=\openm\Sigma^{-1}\mu\\-\frac{1}{2}\flatv(\Sigma^{-1})\closem$ and let $T(x)$ be $\openm x\\\flatv(xx^T)\closem$. Then, we have that
\[\eta^TT(x)=\openm\mu^T\Sigma^{-1}&-\frac{1}{2}\flatv(\Sigma^{-1})^T\closem\openm x\\\flatv(xx^T)\closem=\mu^T\Sigma^{-1}x-\frac{1}{2}\tr(\Sigma^{-1}xx^T)\]
which corresponds to the terms inside the exponent that were not absorbed by $g(\mu,\Sigma)$. Thus, we have that the dimension of $T(x)$ and $\eta$ is $d+d^2$.

In summary, $h(x)=(2\pi)^{-d/2}$, $g(\mu,\Sigma)=|\Sigma^{-1}|\exp\left(-\frac{1}{2}\mu^T\Sigma^{-1}\mu\right)$, $\eta=\openm\Sigma^{-1}\mu\\-\frac{1}{2}\flatv(\Sigma^{-1})\closem$, and $T(x)=\openm x\\\flatv(xx^T)\closem$.
\subsection*{6}
\ssn{a}
If we write $x^{\alpha-1}=e^{(\alpha-1)\log(x)}$, the density of the gamma distribution becomes
\[\frac{1}{\Gamma(\alpha)\beta^\alpha}\exp\left(-\frac{x}{\beta}+(\alpha-1)\log(x)\right)=\frac{1}{\Gamma(\alpha)\beta^\alpha}\exp\left(\openm-\beta^{-1}&\alpha-1\closem\openm x\\\log(x)\closem\right)\]
Thus, we have $\eta=\openm-\beta^{-1}\\\alpha-1\closem$, $T(x)=\openm x\\\log(x)\closem$, $g(\eta)=\frac{1}{\Gamma(\alpha)\beta^\alpha}$, and $h(x)=1$.

By (2.228), the ML estimator for $\eta$ satisfies
\[-\nabla \log g(\eta)=\frac{1}{N}\sum_{i=1}^NT(x_i)\]
In terms of the components of $\eta$, we have $g(\eta)=\frac{(-\eta_1)^{\eta_2+1}}{\Gamma(\eta_2+1)}$. The negative log of this is $\log\Gamma(\eta_2+1)-(\eta_2+1)\log(-\eta_1)$, and its gradient is $\openm\psi^{(0)}(\alpha)-\log\beta\\-\frac{\alpha}{\beta}\closem$, where $\psi^{(0)}$ is the polygamma function.

Then, the pair of equations for $\alpha$ and $\beta$ are 
\begin{align*}
    \psi^{(0)}(\alpha)-\log\beta&=\conj{x}\text{ and}\\
    -\frac{\alpha}{\beta}&=\frac{1}{N}\sum_{i=1}^N\log x_i\\
\end{align*}
Due to the lack of a closed-form inverse for the polygamma function, there probably exists no closed-form solution to this system.
\ssn{b}
The notation got really confusing because $\beta$ is used as a scale parameter here rather than the rate parameter that it usually stands for. I'm just going to flip it around and let $\theta=\beta^{-1}$. If we hold $\alpha$ fixed, we can write the gamma-$\alpha$ distribution with parameter $\theta$ in the exponential family form as
\[\frac{x^{\alpha-1}}{\Gamma(\alpha)}\theta^\alpha e^{\theta(-x)}\]
with $g(\theta)=\theta^\alpha$ and $T(x)=-x$. Then, by (2.229), the conjugate prior for $\theta$ has the form
\[f(\chi, \nu)\theta^{\alpha\nu}e^{\nu\theta\chi}\]
where $\chi$ is a hyperparameter. If we set $\nu=\frac{a-1}{\alpha}$, this has the functional form of a gamma distribution with parameters $a,b$($b$ is a rate parameter), where $b=-\frac{\chi(a-1)}{\alpha}$. Thus, the conjugate prior of $\theta$ is a gamma distribution, which means that the conjugate prior of $\beta$ is an inverse gamma distribution.

If we work in terms of $\theta$, the likelihood for the data $X_i$ is 
\[\frac{\theta^{N\alpha}}{\Gamma(\alpha)^N}\prod_{i=1}^NX_i^{\alpha-1}e^{-X_i\theta}\]
Multiplying this by the conjugate prior produces
\[\left(\frac{\prod_{i=1}^NX_i^{\alpha-1}}{\Gamma(\alpha)^N}\cdot\frac{b^a}{\Gamma(a)}\right)\theta^{N\alpha+a-1}e^{-\theta\sum_{i=1}^NX_i-b\theta}\]
Up to some normalizing constant that absorbs all the stuff in the front in the parens, this is another gamma distribution with shape $a+N\alpha$ and rate $b+\sum X_i$, so the posterior mean of $\theta$ is $\frac{a+N\alpha}{b+\sum X_i}$, which means that the posterior mean of $\beta$ is the reciprocal of this.

\end{document}
