\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}
\subsection*{1}
\ssn{a}
Let $Y=1,\ldots,M$ with $P(Y=m)=\pi_m$, and let $(X|Y=m)\sim N(\mu_m, \sigma_m^2)$. Then, the joint distribution of $X$ and $Y$ is the product of these,
\[f_{X,Y}(x,y)=\phi(x;\mu_y,\sigma_y^2)\pi_y\]
To get the marginal density of $X$ we sum over $Y$, which gives
\[\sum_{m=1}^M\phi(x;\mu_m,\sigma_m^2)\pi_m\]
where $\phi$ is the normal pdf, so this gives us the density for the mixture model.
\ssn{b}
The likelihood for a sample $(x_i,y_i)$ is 
\[\prod_{i=1}^N\phi(x_i;\mu_{y_i}, \sigma_{y_i}^2)\pi(y_i)=\frac{1}{(2\pi)^{N/2}}\prod_{i=1}^N\frac{1}{\sigma_{y_i}}\exp\left(\frac{(x_i-\mu_{y_i})^2}{2\sigma_{y_i}^2}\right)\pi(y_i)\]
whose logarithm is 
\[-\frac{N}{2}\log(2\pi)\sum_{i=1}^N\left(-\log(\sigma_{y_i})+\log\pi(y_i)+\frac{(x_i-\mu_{y_i})^2}{2\sigma_{y_i}^2}\right)\]
Now, separate it by the value of the $Y_i$, letting $N_m$ be the number of observations with $Y=m$. Doing this and discarding the constant factor in the front, we get
\[\sum_{m=1}^MN_m\left(\log\pi_m-\log\sigma_m\right)+\sum_{i=1}^{N_m}\frac{(x_i-\mu_m)^2}{2\sigma_m^2}\]
Differentating wrt $\pi_m$ and setting equal to $0$, we obtain $\h{\pi_m}=N_m$, which after normalizing so that their sum is $1$, becomes $\h{\pi_m}=\frac{N_m}{N}$. Then, what we obtain is
\[\sum_{m=1}^M\sum_{i=1}^{N_m}\frac{(x_i-\mu_m)^2}{2\sigma_m^2}-\log\sigma_m+\h{\pi_m}\]
Then, we just have a collection of $M$ unrelated normal log-likelihoods to maximize, so the MLE of the means and the variances are just the sample mean and sample variance within the classes.
\ssn{c}
The likelihood of the data $X_i$ (ignoring the constant multipliers that don't matter for maximization) is
\[\prod_{i=1}^N\frac{\pi_1}{\sigma_1}\exp\left(-\frac{(X_i-\mu_1)^2}{2\sigma_1^2}\right)+\frac{\pi_2}{\sigma_2}\exp\left(-\frac{(X_i-\mu_2)^2}{2\sigma_2^2}\right)\]
and taking the log gives
\[\sum_{i=1}^N\log\left(\frac{\pi_1}{\sigma_1}\exp\left(-\frac{(X_i-\mu_1)^2}{2\sigma_1^2}\right)+\frac{\pi_2}{\sigma_2}\exp\left(-\frac{(X_i-\mu_2)^2}{2\sigma_2^2}\right)\right)\]
If we plug in the given estimates for the parameters apart from the $\sigma$s, and assume that $\sigma_2$ is fixed, then examine the behaviour of the first term inside the log for each $i$ as $\h{\sigma}_1\to0$. If $i\neq1$, then the expression inside the exponential will decay to $0$ extremely quickly, dominating the $\frac{1}{\sigma_1}$ term outside the exponential. However, for $i=1$, the term inside the exponential is zero, which means that the first term becomes $\frac{\pi_1}{\sigma_1}$, and this will go to infinity as $\sigma_1\to0$. Thus, the whole log-likelihood will also diverge to infinity. This means that if we try to use the EM algorithm on this and one of the estimates of the means ends up falling on one of the data points exactly, it'll start to diverge because the likelihood has no global maximum.
\ssn{d}
For the $i\neq1$ terms, setting $\sigma_1=\sigma_2=\sigma$ and letting that go to zero in the log-likelihood produces $N-1$ terms that asymptotically behaves like $\log\left(\frac{1}{\sigma}\exp(-\sigma^{-2})\right)$, and the $i=1$ term still behaves like $\log\sigma^{-1}$. Adding these together results in the exponential terms decaying much faster thatn the $\sigma^{-1}$ terms, which means that the log-likelihood will go to $-\infty$ instead, avoiding the problem.
\ssn{e}
Continuing from (b), after estimating the $\pi_m$, we obtain 
\[\sum_{m=1}^MN_m\left(\log\h{\pi}_m-\log\sigma_m\right)+\sum_{i=1}^{N_m}\frac{(x_i-\mu_m)^2}{2\sigma_m^2}\]
for the log-likelihood. Differentiating wrt $\mu_1$ and setting equal to zero, we have that $\h{\mu}_m$ is the sample mean of the $X_i$ with corresponding $Y_i$ equal to $m$, for $m=1,2$.

Finally, note that this is now the same as the $2$-class classification problem whose MLE we computed in problem 3 of the last homework. From there, we can conclude that the MLE of the covariance is the weighted average of the sample covariances, with 
\ssn{f}
Begin with initial estimates $\mu_1^{(0)}$, $\mu_2^{(0)}$, $\sigma^{(0)}$, $\pi_1^{(0)}$, and $\pi_2^{(0)}$. Collectively, call these parameters $\eta$. In the $k$th step, let $w_{n, m}=f(Y=m|X_n;\eta^{(k-1)})$ for $m=1,2$. Estimate $\pi_m^{(k)}$ by taking $\frac{1}{N}\sum_{i=1}^Nw_{m, i}$. Then, set
\begin{align*}
    \mu_1^{(k)}&=\frac{\frac{1}{N}\sum_{i=1}^Nw_{1,i}X^{(i)}}{\sum_{i=1}^Nw_{1, i}}\\
    \mu_2^{(k)}&=\frac{\frac{1}{N}\sum_{i=1}^Nw_{2,i}X^{(i)}}{\sum_{i=1}^Nw_{2, i}}\\
    \sigma^{(k)}&=\pi_1^{(k)}\frac{

\end{document}
