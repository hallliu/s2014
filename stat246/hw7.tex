\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\usepackage{mathrsfs}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}

\subsection*{1}
\ssn{a}
Suppose $j$ is a neighbor of $i$. Then, there exists some clique that contains both $i$ and $j$, so $j$ is in the union of the cliques containing $i$. Conversely, suppose $j$ is in the union of cliques containing $i$. Then, $i$ and $j$ coexist in a clique together, so they have an edge between them, which means that $j$ is a neighbor of $i$.
\ssn{b}
Denote the set of cliques which contain $X_i$ by $C(i)$ and its complement in the set of cliques by $C(-i)$. Then, we have that $P(X_i|X_{N_i})=P(X_i|X_{-i})$ (where $X_{-i}$ is all nodes except $X_i$), which can be written as 
\[\frac{\exp\left(\beta\sum_l\psi_{C_l}(X_{C_l})\right)}{\exp\left(\beta\sum_{C\in C(-i)}\psi_C(X_C)\right)\sum_{X_i}\exp\left(\beta\sum_{C\in C(i)}\psi_C(X_C)\right)}=\frac{\exp\left(\beta\sum_{C\in C(i)}\psi_{C}(X_{C})\right)}{\sum_{X_i}\exp\left(\beta\sum_{C\in C(i)}\psi_C(X_C)\right)}\]
Using this, then, we can write the log-pseudolikelihood as
\[\sum_{n=1}^N\sum_{i=1}^d\left(\beta\sum_{C\in C(i)}\psi_{C}(X_{C})-\log\left(\sum_{X_i}\exp\left(\beta\sum_{C\in C(i)}\psi_C(X_C)\right)\right)\right)\]
where the summation over $n$ operates via an implicit superscript $(n)$ over each datum $X$. Differentiating wrt $\beta$ and setting to $0$ gives 
\[0=\sum_{n=1}^N\sum_{i=1}^d\left(\sum_{C\in C(i)}\psi_{C}(X_{C})-\frac{\sum_{X_i}\left(\sum_{C\in C(i)}\psi_C(X_C)\right)\exp\left(\beta\sum_{C\in C(i)}\psi_C(X_C)\right)}{\sum_{X_i}\exp\left(\beta\sum_{C\in C(i)}\psi_C(X_C)\right)}\right)\]
It doesn't look like this can be simplified any further without knowing the forms of the cliques.
\ssn{c}
Let $f(X_i,X_{N_i})=\sum_{C\in C(i)}\psi_C(X_C)$. Then, we can rewrite the equation from (b) as
\[\sum_{n=1}^N\sum_{i=1}^df(X_i, X_{N_i})=\sum_{n=1}^N\sum_{i=1}^d\frac{\sum_{x_i}\left(f(x_i,X_{N_i})\exp\left(\beta f(x_i,X_{N_i})\right)\right)}{\sum_{x_i}\exp\left(\beta f(x_i,X_{N_i})\right)}\]
First, examine the LHS. If we look at the sum as being over the different possible configurations of $X_i$ and $X_{N_i}$ rather than being over the data points, we can rewrite it as
\[\sum_{i=1}^d\sum_{x_{N_i}}\sum_{x_i}f(x_i, x_{N_i})\left|\{n:X_i^{(n)}=x_i,X_{N_i}^{(n)}=x_{N_i}\}\right|\]
where the multiplier is the number of data points which have the configuration specified by $x_i$ and $x_{N_i}$. We can then express this in terms of the empirical expectation as 
\[\sum_{i=1}^d\sum_{x_{N_i}}\left|\{n:X_{N_i}=x_{N_i}\}\right| E^{(emp)}(f(X_i,X_{N_i})|X_{N_i}=x_{N_i})\]

To pare down the right side, first note that the (non-empirical) expectation is given by
\[E(f(X_i,X_{N_i})|X_{N_i}=x_{N_i})=\sum_{x_i}f(x_i,x_{N_i})P(x_i|x_{N_i})=\sum_{x_i}f(x_i, x_{N_i})\frac{\exp\left(\beta f(x_i,x_{N_i})\right)}{\sum_{x_i'}\exp\left(\beta f(x_i',x_{N_i})\right)}\]
Conveniently, this is the same as the inner summand of the RHS, so we can write the RHS as
\[\sum_{i=1}^d\sum_{n=1}^NE(f(X_i^{(n)},X_{N_i}^{(n)})|X_{N_i})=\sum_{i=1}^d\sum_{x_{N_i}}E(f(X_i,X_{N_i})|X_{N_i}=x_{N_i})\left|\{n:X_{N_i}=x_{N_i}\}\right|\]
Then, trimming away the leftover counts (which are attached to the same terms on both sides) gives us the desired equality.
\ssn{d}
In the same vein as (b), the log-pseudolikelihood is
\[\sum_{n=1}^N\sum_{i=1}^d\left(\sum_{C\in C(i)}\beta_C\psi_{C}(X_{C})-\log\left(\sum_{X_i}\exp\left(\sum_{C\in C(i)}\beta_C\psi_C(X_C)\right)\right)\right)\]
Fix a clique $C$ and differentiate wrt $\beta_C$ to obtain
\[\sum_{n=1}^N\sum_{i\in C}\left(\psi_C(X_C)-\frac{\sum_{x_i}\psi_C(X_C)\exp\left(\sum_{C'\in C(i)}\beta_{C'}\psi_{C'}(X_{C'})\right)}{\sum_{x_i}\exp\left(\sum_{C'\in C(i)}\beta_{C'}\psi_{C'}(X_{C'})\right)}\right)=0\]
Then, going through the same steps as in (c), we see that the result is the same, but with $f(x_i,X_{N_i})=\psi_C(X_C)$ for every clique $C$ that contains $i$.
\subsection*{2}

\end{document}
