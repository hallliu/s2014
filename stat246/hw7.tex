\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\usepackage{mathrsfs}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}

\subsection*{1}
\ssn{a}
Suppose $j$ is a neighbor of $i$. Then, there exists some clique that contains both $i$ and $j$, so $j$ is in the union of the cliques containing $i$. Conversely, suppose $j$ is in the union of cliques containing $i$. Then, $i$ and $j$ coexist in a clique together, so they have an edge between them, which means that $j$ is a neighbor of $i$.
\ssn{b}
Denote the set of cliques which contain $X_i$ by $C(i)$ and its complement in the set of cliques by $C(-i)$. Then, we have that $P(X_i|X_{N_i})=P(X_i|X_{-i})$ (where $X_{-i}$ is all nodes except $X_i$), which can be written as 
\[\frac{\exp\left(\beta\sum_l\psi_{C_l}(X_{C_l})\right)}{\exp\left(\beta\sum_{C\in C(-i)}\psi_C(X_C)\right)\sum_{X_i}\exp\left(\beta\sum_{C\in C(i)}\psi_C(X_C)\right)}=\frac{\exp\left(\beta\sum_{C\in C(i)}\psi_{C}(X_{C})\right)}{\sum_{X_i}\exp\left(\beta\sum_{C\in C(i)}\psi_C(X_C)\right)}\]
Using this, then, we can write the log-pseudolikelihood as
\[\sum_{n=1}^N\sum_{i=1}^d\left(\beta\sum_{C\in C(i)}\psi_{C}(X_{C})-\log\left(\sum_{X_i}\exp\left(\beta\sum_{C\in C(i)}\psi_C(X_C)\right)\right)\right)\]
where the summation over $n$ operates via an implicit superscript $(n)$ over each datum $X$. Differentiating wrt $\beta$ and setting to $0$ gives 
\[0=\sum_{n=1}^N\sum_{i=1}^d\left(\sum_{C\in C(i)}\psi_{C}(X_{C})-\frac{\sum_{X_i}\left(\sum_{C\in C(i)}\psi_C(X_C)\right)\exp\left(\beta\sum_{C\in C(i)}\psi_C(X_C)\right)}{\sum_{X_i}\exp\left(\beta\sum_{C\in C(i)}\psi_C(X_C)\right)}\right)\]
It doesn't look like this can be simplified any further without knowing the forms of the cliques.
\ssn{c}
Let $f(X_i,X_{N_i})=\sum_{C\in C(i)}\psi_C(X_C)$. Then, we can rewrite the equation from (b) as
\[\sum_{n=1}^N\sum_{i=1}^df(X_i, X_{N_i})=\sum_{n=1}^N\sum_{i=1}^d\frac{\sum_{x_i}\left(f(x_i,X_{N_i})\exp\left(\beta f(x_i,X_{N_i})\right)\right)}{\sum_{x_i}\exp\left(\beta f(x_i,X_{N_i})\right)}\]
First, examine the LHS. If we look at the sum as being over the different possible configurations of $X_i$ and $X_{N_i}$ rather than being over the data points, we can rewrite it as
\[\sum_{i=1}^d\sum_{x_{N_i}}\sum_{x_i}f(x_i, x_{N_i})\left|\{n:X_i^{(n)}=x_i,X_{N_i}^{(n)}=x_{N_i}\}\right|\]
where the multiplier is the number of data points which have the configuration specified by $x_i$ and $x_{N_i}$. We can then express this in terms of the empirical expectation as 
\[\sum_{i=1}^d\sum_{x_{N_i}}\left|\{n:X_{N_i}=x_{N_i}\}\right| E^{(emp)}(f(X_i,X_{N_i})|X_{N_i}=x_{N_i})\]

To pare down the right side, first note that the (non-empirical) expectation is given by
\[E(f(X_i,X_{N_i})|X_{N_i}=x_{N_i})=\sum_{x_i}f(x_i,x_{N_i})P(x_i|x_{N_i})=\sum_{x_i}f(x_i, x_{N_i})\frac{\exp\left(\beta f(x_i,x_{N_i})\right)}{\sum_{x_i'}\exp\left(\beta f(x_i',x_{N_i})\right)}\]
Conveniently, this is the same as the inner summand of the RHS, so we can write the RHS as
\[\sum_{i=1}^d\sum_{n=1}^NE(f(X_i^{(n)},X_{N_i}^{(n)})|X_{N_i})=\sum_{i=1}^d\sum_{x_{N_i}}E(f(X_i,X_{N_i})|X_{N_i}=x_{N_i})\left|\{n:X_{N_i}=x_{N_i}\}\right|\]
Then, trimming away the leftover counts (which are attached to the same terms on both sides) gives us the desired equality.
\ssn{d}
In the same vein as (b), the log-pseudolikelihood is
\[\sum_{n=1}^N\sum_{i=1}^d\left(\sum_{C\in C(i)}\beta_C\psi_{C}(X_{C})-\log\left(\sum_{X_i}\exp\left(\sum_{C\in C(i)}\beta_C\psi_C(X_C)\right)\right)\right)\]
Fix a clique $C$ and differentiate wrt $\beta_C$ to obtain
\[\sum_{n=1}^N\sum_{i\in C}\left(\psi_C(X_C)-\frac{\sum_{x_i}\psi_C(X_C)\exp\left(\sum_{C'\in C(i)}\beta_{C'}\psi_{C'}(X_{C'})\right)}{\sum_{x_i}\exp\left(\sum_{C'\in C(i)}\beta_{C'}\psi_{C'}(X_{C'})\right)}\right)=0\]
Then, going through the same steps as in (c), we see that the result is the same, but with $f(x_i,X_{N_i})=\psi_C(X_C)$ for every clique $C$ that contains $i$.
\subsection*{2}
Performing the multiplication $P_nP=P_{n+1}$ to get a recurrence, we have
\[\openm1-a_n&a_n\\b_n&1-b_n\closem\openm1-a&a\\b&1-b\closem=\openm1-(a+a_n(1-a-b))&a+a_n(1-a-b)\\b+b_n(1-a-b)&1-(b+b_n(1-a-b))\closem\]
so the recurrence is $a_{n+1}=a+a_n(1-a-b)$ and similar for $b_{n+1}$. To find the steady-state solution, set $a_n=a_{n+1}=a^*$ to obtain $a^*=a+a^*(1-a-b)$ or $a^*=\frac{a}{a+b}$ (and similarly $b^*=\frac{b}{a+b}$). This gives us the limit of $P^n$ as 
\[\openm\frac{b}{a+b}&\frac{a}{a+b}\\\frac{b}{a+b}&\frac{a}{a+b}\closem\]
To find closed-form formulas for $a_n$ and $b_n$, note that $a_{n+1}-a^*=(a_n-a^*)(1-a-b)$, or $a_n-a^*=(1-a-b)^{n-1}(a-a^*)$. The same goes for $b_n$.
\subsection*{3}
\ssn{a}
\vspace{130pt}
\ssn{b}
To construct $Q$ (for $n\geq3$), start with a tridiagonal matrix with $\beta$ on the diagonal and $-\alpha$ on the off-diagonals, then place $-\alpha$ at the upper-right and lower-left corners. Then, taking $x^TQx$ results in 
\[\sum_{i=1}^nx_i(-\alpha x_{i-1}+\beta x_i-\alpha x_{i+1})\]
which simplifies to $\sum_{i=1}^n-2\alpha x_ix_{i+1}+\beta x_i^2$ when we combine the like terms across summands. Multiplying by $-\frac{1}{2}$ gives us the term in the exponent.
\ssn{c}
The $l$th entry of $Qv_k$ is $\frac{\beta}{\sqrt{n}}\exp(2\pi i\frac{l}{n}k)-\frac{\alpha}{\sqrt{n}}\left(\exp(2\pi i\frac{l-1}{n}k)+\exp(2\pi i\frac{l+1}{n}k)\right)$. This is a real multiple of $\exp(2\pi i\frac{l}{n}k)$ since adding the two $\alpha$-multiplied vectors results in something with the same argument. For $k$ fixed, the multiplier is the same for every $l$, and its value is $\frac{\beta-2\alpha\cos(2\pi k/n)}{\sqrt{n}}$. Thus, the $v_k$ are eigenvectors. Further, since we know that $\beta>2\alpha$, all the eigenvalues are positive, which implies that $Q$ is strictly positive definite.
\ssn{d}
Since this density is invariant under cyclically permuting the variables, all that's needed is to find the distribution of $x_1$ conditioned on all the others. Write the covariance $Q$ as $\openm \beta&Q_{12}\\Q_{21}&Q_{22}\closem$. Then, if the other variables taken together equal some $n-1$-long vector $y$, the conditional mean of $x_1$ is $Q_{12}Q_{22}^{-1}y$ and the conditional variance is $\beta-Q_{12}Q_{22}^{-1}Q_{21}$. For all the variables, $Q_{22}$ will be a $n-1\times n-1$ tridiagonal matrix having $\beta$ on the diagonal and $-\alpha$ on the off-diagonal.
\end{document}
