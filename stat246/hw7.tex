\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\usepackage{mathrsfs}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}

\subsection*{1}
\ssn{a}
Suppose $j$ is a neighbor of $i$. Then, there exists some clique that contains both $i$ and $j$, so $j$ is in the union of the cliques containing $i$. Conversely, suppose $j$ is in the union of cliques containing $i$. Then, $i$ and $j$ coexist in a clique together, so they have an edge between them, which means that $j$ is a neighbor of $i$.
\ssn{b}
Denote the set of cliques which contain $X_i$ by $C(i)$ and its complement in the set of cliques by $C(-i)$. Then, we have that $P(X_i|X_{N_i})=P(X_i|X_{-i})$ (where $X_{-i}$ is all nodes except $X_i$), which can be written as 
\[\frac{\exp\left(\beta\sum_l\psi_{C_l}(X_{C_l})\right)}{\exp\left(\beta\sum_{C\in C(-i)}\psi_C(X_C)\right)\sum_{X_i}\exp\left(\beta\sum_{C\in C(i)}\psi_C(X_C)\right)}=\frac{\exp\left(\beta\sum_{C\in C(i)}\psi_{C}(X_{C})\right)}{\sum_{X_i}\exp\left(\beta\sum_{C\in C(i)}\psi_C(X_C)\right)}\]
Using this, then, we can write the log-pseudolikelihood as
\[\sum_{n=1}^N\sum_{i=1}^d\left(\beta\sum_{C\in C(i)}\psi_{C}(X_{C})-\log\left(\sum_{X_i}\exp\left(\beta\sum_{C\in C(i)}\psi_C(X_C)\right)\right)\right)\]
where the summation over $n$ operates via an implicit superscript $(n)$ over each datum $X$. Differentiating wrt $\beta$ and setting to $0$ gives 
\[0=\sum_{n=1}^N\sum_{i=1}^d\left(\sum_{C\in C(i)}\psi_{C}(X_{C})-\frac{\sum_{X_i}\left(\sum_{C\in C(i)}\psi_C(X_C)\right)\exp\left(\beta\sum_{C\in C(i)}\psi_C(X_C)\right)}{\sum_{X_i}\exp\left(\beta\sum_{C\in C(i)}\psi_C(X_C)\right)}\right)\]
It doesn't look like this can be simplified any further without knowing the forms of the cliques.
\ssn{c}
Let $f(X_i,X_{N_i})=\sum_{C\in C(i)}\psi_C(X_C)$. Then, we can rewrite the equation from (b) as
\[\sum_{n=1}^N\sum_{i=1}^df(X_i, X_{N_i})=\sum_{n=1}^N\sum_{i=1}^d\frac{\sum_{x_i}\left(f(x_i,X_{N_i})\exp\left(\beta f(x_i,X_{N_i})\right)\right)}{\sum_{x_i}\exp\left(\beta f(x_i,X_{N_i})\right)}\]
First, examine the LHS. If we look at the sum as being over the different possible configurations of $X_i$ and $X_{N_i}$ rather than being over the data points, we can rewrite it as
\[\sum_{i=1}^d\sum_{x_{N_i}}\sum_{x_i}f(x_i, x_{N_i})\left|\{n:X_i^{(n)}=x_i,X_{N_i}^{(n)}=x_{N_i}\}\right|\]
where the multiplier is the number of data points which have the configuration specified by $x_i$ and $x_{N_i}$. We can then express this in terms of the empirical expectation as 
\[\sum_{i=1}^d\sum_{x_{N_i}}\left|\{n:X_{N_i}=x_{N_i}\}\right| E^{(emp)}(f(X_i,X_{N_i})|X_{N_i}=x_{N_i})\]

To pare down the right side, first note that the (non-empirical) expectation is given by
\[E(f(X_i,X_{N_i})|X_{N_i}=x_{N_i})=\sum_{x_i}f(x_i,x_{N_i})P(x_i|x_{N_i})=\sum_{x_i}f(x_i, x_{N_i})\frac{\exp\left(\beta f(x_i,x_{N_i})\right)}{\sum_{x_i'}\exp\left(\beta f(x_i',x_{N_i})\right)}\]
Conveniently, this is the same as the inner summand of the RHS, so we can write the RHS as
\[\sum_{i=1}^d\sum_{n=1}^NE(f(X_i^{(n)},X_{N_i}^{(n)})|X_{N_i})=\sum_{i=1}^d\sum_{x_{N_i}}E(f(X_i,X_{N_i})|X_{N_i}=x_{N_i})\left|\{n:X_{N_i}=x_{N_i}\}\right|\]
Thus we have the desired equality (after the Monday correction).
\ssn{d}
In the same vein as (b), the log-pseudolikelihood is
\[\sum_{n=1}^N\sum_{i=1}^d\left(\sum_{C\in C(i)}\beta_C\psi_{C}(X_{C})-\log\left(\sum_{X_i}\exp\left(\sum_{C\in C(i)}\beta_C\psi_C(X_C)\right)\right)\right)\]
Fix a clique $C$ and differentiate wrt $\beta_C$ to obtain
\[\sum_{n=1}^N\sum_{i\in C}\left(\psi_C(X_C)-\frac{\sum_{x_i}\psi_C(X_C)\exp\left(\sum_{C'\in C(i)}\beta_{C'}\psi_{C'}(X_{C'})\right)}{\sum_{x_i}\exp\left(\sum_{C'\in C(i)}\beta_{C'}\psi_{C'}(X_{C'})\right)}\right)=0\]
Then, going through the same steps as in (c), we see that the result is the same, but with $f(x_i,X_{N_i})=\psi_C(X_C)$ for every clique $C$ that contains $i$.
\subsection*{2}
Performing the multiplication $P_nP=P_{n+1}$ to get a recurrence, we have
\[\openm1-a_n&a_n\\b_n&1-b_n\closem\openm1-a&a\\b&1-b\closem=\openm1-(a+a_n(1-a-b))&a+a_n(1-a-b)\\b+b_n(1-a-b)&1-(b+b_n(1-a-b))\closem\]
so the recurrence is $a_{n+1}=a+a_n(1-a-b)$ and similar for $b_{n+1}$. To find the steady-state solution, set $a_n=a_{n+1}=a^*$ to obtain $a^*=a+a^*(1-a-b)$ or $a^*=\frac{a}{a+b}$ (and similarly $b^*=\frac{b}{a+b}$). This gives us the limit of $P^n$ as 
\[\openm\frac{b}{a+b}&\frac{a}{a+b}\\\frac{b}{a+b}&\frac{a}{a+b}\closem\]
To find closed-form formulas for $a_n$ and $b_n$, note that $a_{n+1}-a^*=(a_n-a^*)(1-a-b)$, or $a_n-a^*=(1-a-b)^{n-1}(a-a^*)$. The same goes for $b_n$.
\subsection*{3}
\ssn{a}
\vspace{130pt}
\ssn{b}
To construct $Q$ (for $n\geq3$), start with a tridiagonal matrix with $\beta$ on the diagonal and $-\alpha$ on the off-diagonals, then place $-\alpha$ at the upper-right and lower-left corners. Then, taking $x^TQx$ results in 
\[\sum_{i=1}^nx_i(-\alpha x_{i-1}+\beta x_i-\alpha x_{i+1})\]
which simplifies to $\sum_{i=1}^n-2\alpha x_ix_{i+1}+\beta x_i^2$ when we combine the like terms across summands. Multiplying by $-\frac{1}{2}$ gives us the term in the exponent.
\ssn{c}
The $l$th entry of $Qv_k$ is $\frac{\beta}{\sqrt{n}}\exp(2\pi i\frac{l}{n}k)-\frac{\alpha}{\sqrt{n}}\left(\exp(2\pi i\frac{l-1}{n}k)+\exp(2\pi i\frac{l+1}{n}k)\right)$. This is a real multiple of $\exp(2\pi i\frac{l}{n}k)$ since adding the two $\alpha$-multiplied vectors results in something with the same argument. For $k$ fixed, the multiplier is the same for every $l$, and its value is $\frac{\beta-2\alpha\cos(2\pi k/n)}{\sqrt{n}}$. Thus, the $v_k$ are eigenvectors. Further, since we know that $\beta>2\alpha$, all the eigenvalues are positive, which implies that $Q$ is strictly positive definite.
\ssn{d}
The conditional variance of $x_i$ conditioned upon all the others is $\frac{1}{Q_{ii}}$, and the conditional mean is $-\frac{1}{Q_{ii}}Q_i^Tx$, with the $i$th coordinate being excluded from the inner product.
\ssn{e}
Plots of the trajectories for $\ep=0.01$:

\includegraphics[width=0.9\textwidth]{hw7_files/traj_e2.png}

It looks like things converged, since none of the coordinates are moving around much. On the other hand, for $\ep=0.00001$, the chain definitely has not converged to the stationary distribution.

\includegraphics[width=0.9\textwidth]{hw7_files/traj_e5.png}

Calculating $\rho$ from the estimated covariance matrices, at $\ep=0.00001$, the first 1000 yields $\rho=0.9693$, the last 1000 yields $\rho=1.034$, and the entire sample yields $\rho=0.971$. At $\ep=0.01$, the figures are $0.265$, $-0.291$, and $0.0157$ in the same order.
\ssn{f}
Histograms follow. $\ep=0.01$ is on the left.

\includegraphics[width=0.5\textwidth]{hw7_files/e2_hist.png}
\includegraphics[width=0.5\textwidth]{hw7_files/e5_hist.png}
\ssn{g}
Same order as above.

\includegraphics[width=0.5\textwidth]{hw7_files/e2_direct_hist.png}
\includegraphics[width=0.5\textwidth]{hw7_files/e5_direct_hist.png}

For the $\ep=0.00001$ case, direct sampling is a lot better, but this comes at the cost of having to do a Cholesky on the precision matrix and inverting the resulting upper-triangular matrix.
\ssn{h}
If $x_i$ is always either $1$ or $-1$, then $-\frac{1}{2}\beta x_i^2$ is just $-\frac{\beta}{2}$, so we can write 
\[f\propto\exp\left(\sum_{i=1}^n\alpha x_ix_{i+1}-\frac{\beta}{2}\right)=\exp\left(-\frac{\beta}{2}\right)\exp\left(\sum_{i=1}^n\alpha x_ix_{i+1}\right)\propto\exp\left(\sum_{i=1}^n\alpha x_ix_{i+1}\right)\]
so $\beta$ is irrelevant to the distribution.

The conditional is simply the joint distribution divided by the marginal over everything but $x_i$, which can be found by evaluating the joint distribution at $x_i=1$ and at $x_i=-1$. For $x_1$, this marginal is
\[\exp\left(\alpha(x_2+x_2x_3+x_3x_4+x_4)\right)+\exp\left(\alpha(-x_2+x_2x_3+x_3x_4-x_4)\right)\]

The actual probabilities are given in the table below ($-1$s are notated as zeros for readability):

{\tiny
    \noindent
\begin{tabular}{r*{16}{c}}
    0000&0001&0010&0011&0100&0101&0110&0111&1000&1001&1010&1011&1100&1101&1110&1111\\
    $0.45$&$0.00825$&$0.00825$&$0.00825$&$0.00825$&$0.000151$&$0.00825$&$0.00825$&$0.00825$&$0.00825$&$0.000151$&$0.00825$&$0.00825$&$0.00825$&$0.00825$&$0.45$\\
\end{tabular}
}

Most likely states are the $0000$ state and the $1111$ state.

A plot of trajectories is shown below, with the components not labeled nor differentiated. There are 16 lines, each representing the proportion of its corresponding component. The $x$-axis indicates the number of samples taken.

\includegraphics[width=0.6\textwidth]{hw7_files/bin_traj.png}

Looks like the empirical frequencies are converging to the true probabilities.
\ssn{i}
The sign in front of the $\beta$ term doesn't matter, since we showed above that the value of $\beta$ doesn't even matter. $\Sigma$ is the same as $Q$ from part (b), except the $\alpha$s no longer carry a negative sign with them. This matrix is still positive definite, since if $\beta>2\alpha$, it is strictly diagonally dominant.

The integral given is the expectation of $\exp(x^Ty)$ under the distribution $g$, which is the moment-generating function evaluated at $x$. The mgf of $g$ is $\exp\left(\frac{1}{2}x^T\Sigma x\right)$, which is proportional to $f$ from above.

Given this, another possible way to sample from $f$ would be to sample first from $g$, then convert all the positive entries to $1$ and all the negative ones to $-1$.
\end{document}
