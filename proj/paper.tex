\documentclass[psamsfonts]{amsart}

\usepackage{amssymb,amsfonts}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{hyperref}

\makeatletter
\makeatother

\bibliographystyle{plain}
\title{A distributed key-value store using Raft}

\author{Hall Liu}

\begin{document}

\maketitle
\section{Summary of the protocol}
This implementation of a distributed, fault-tolerant key-value store is based on the Raft algorithm by Ongaro and Ousterhout \footnote{Diego Ongaro, John Ousterhout; \href{https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf}{In search of an Understandable Consensus Algorithm}. 2014}. The Raft algorithm is an algorithm which implements a fault-tolerant replicated state machine, where in this case the state to be replicated is the dictionary containing the keys and values that are to be stored. In this section, I will give a brief summary of the fundamentals of the algorithm.

Each node participating in a Raft cluster has some internal state which is distinct from the state of the key-value store. The former will be referred to as the \emph{Raft state}, and the latter will be referred to as the \emph{replicated state}. 
\subsection{Roles of the nodes}
Each node in a Raft cluster may take one of three roles: {\it leader}, {\it follower}, or {\it candidate}. The leader is the node responsible for coordinating communication between the nodes and client. In normal operation, the leader receives all messages coming from outside the cluster, and it is the recipient of all messages originating with other nodes in the cluster. All other nodes are followers, which keep a copy of the leader's replicated state and update it in accordance with the leader's instructions. The candidate state is invoked when communication with the leader is disrupted for some reason -- it is the transitional state that a follower goes into in order to become the new leader of the cluster.
\subsection{Terms and Leader election}
The leader of a Raft cluster is elected for the duration of a {\it term}, which is a part of the Raft state that the algorithm aims to keep consistent across all nodes. The term is a monotonically increasing variable which is included in each message that is passed from one node to another. Upon receiving any message, a node will verify that its term matches the term of the sender. If its own term is lower, it will update its term to the term of the sender, fall back to a follower state, and wait for instructions from any emerging leaders. If its own term is higher, the message will typically be disregarded, except in the case where the leader receives an outdated reply from a follower.

During normal operation, in the absence of faults or significant delays, a term will last indefinitely. However, in the event of a fault, each follower has an internal timeout which triggers after $t$ seconds without contact from the leader node, where $t$ is drawn randomly between $150$ms and $300$ms. Upon reaching this timeout, a follower will increment its term number and send a vote request to all its peers. All nodes carry who they voted for in any term, and it will grant a vote upon request if the requester is at least up-to-date as it is. Then, once a candidate receives votes from a majority of its peers (including itself -- a candidate immediately casts a vote for itself upon becoming a candidate), it will become a leader which has a log at least as up-to-date as a majority of the cluster.
\subsection{The Raft log and AppendEntries messages}
Once elected into office, the leader communicates with all other nodes in the cluster through \emph{AppendEntries} messages (these are RPCs in the original paper, but this implementation uses message-passing for communication). These messages are the heart of the algorithm -- they carry information about the leader's replicated state and also quell the followers' instinct to rebel. 

The leader is the recipient of all requests from clients to modify the replicated state of the cluster. To that purpose, it encodes all such requests as a sequence of commands that may be applied to the replicated state, and stores these commands in sequence in the \emph{Raft log}. The Raft log is the mechanism by which followers keep their replicated state consistent -- if two logs agree, then the state obtained by applying the logs to the replicated state must also agree.

AppendEntries messages, then, are requests from the leader to followers to append entries to their copy of the Raft log. In order for the log to be kept consistent, each entry in the log is recorded with the term during which it was generated by the leader. Each AppendEntries message contains the index and the term of the entry immediately prior (in the leader's log) to the first entry enclosed with the message. If the follower's log contains an entry at that index with that term, it will overwrite its own log past that point with the entries contained in the message. If not, it will respond with an error message.

AppendEntries also carry information about when to commit Raft log entries to a node's copy of the replicated state. Since committed changes are assumed to be irreversible, the leader will only instruct followers to commit an entry after it knows that a majority of the followers have the entry in their log in the proper index. This way, even if faults happen, any newly elected leader must also have this entry in their log, as otherwise they would not receive votes from a majority due to being out-of-date. The commit information is carried as an integer in each AppendEntries message -- the leader sends the index of the last entry that it has committed into its replicated state.

Finally, AppendEntries messages also serve to keep the leader in power. Whenever a node receives an AppendEntries message with a term at least as high as its own, it will revert back to follower status and reset its timeout. Thus, if a leader sends out AppendEntries messages (possibly with zero actual entries attached) with greater frequency than once every $150$ms, no other nodes will attempt to become leader.
\section{Description of the implementation}
\subsection{Structure and requests}
The actual implementation is done in Python building on the provided example node as a base. There are two distinct components to this implementation -- the client node and the server node. The server node code implements the Raft state and its transitions and stores the replicated state of the cluster. The client node serves as a middleman between the cluster and the broker due to the broker's limited abilities to address nodes. A typical run with the broker scripts will start the client nodes first, then the server nodes.

\subsection{GET requests}
When the broker issues a get request, it should always be directed towards one of the clients. Upon receiving this request, the client node associates the info in the request to the id provided by the broker so that it can formulate a response back to the broker after the Raft cluster responds. It then generates a new id which is unique with respect to all other clients and requests by prepending its name to a counter incremented every time an id is generated. The client node will then associate this new id with the broker's id for the request, then send off a \verb|GET| message to who it thinks the Raft leader is.

The client node initially assumes that the first entry in its list of Raft nodes is the leader. However, if it sends a \verb|GET| message to a node which is not the leader, that node will reply with a \verb|redirect| message providing who it thinks that the current leader is. The client will then record this as the new leader and retry the request. If the cluster is currently transitioning between leaders, the response to the \verb|redirect| may be null. In this case, the client retries with a randomly chosen node.

Due to the nature of log replication, the leader will always have the most up-to-date replicated state if it is still the sole leader and has not been deposed. Thus, there is no need for the leader to append an entry into its log -- it can just respond using its copy of the replicated state after verifying that it has not been deposed. To do this, after receiving the \verb|GET| request from the client, the cluster leader sends out a possibly empty AppendEntries message tagged with the client's request id to all followers in order to solicit a response. It then associates the request id to client request, then exits to process other messages. The leader will keep track of the responses associated with that id, and once it receives a majority affirming that the term has not ended, it will reply back to the client with the appropriate data indicating success, including the client's request id that was sent with the initial request. When the client receives this message, it will retrieve the broker's request using the id in the reply, then formulate a reply back to the broker indicating success and carrying the value.

However, if a fault occurs during this process which leaves the leader unable to contact a majority of nodes, no response may be generated. To handle this case, the client nodes associate a $600$ms timeout with each request, and will retry the request if this timeout expires. If the timeout expires, the client will pick a node at random to contact, because the assumption is that the timeout is due to the leader having gone down in some fashion. The value of the timeout was chosen to be small while still being long enough to wait out leadership changes. After some number of retries (I picked $5$ to allow for two cycles of timeout-redirects), the client will return an error to the broker. 
\subsection{SET requests}
The beginning and end of a \verb|SET| request look much like those of \verb|GET| requests, wherein the broker submits a request to the client, the client repackages it for the cluster, and the response follows the same chain backwards. The middle part of a \verb|SET| request differs, as setting a value requires a change to be committed into the replicated state. To do this, the leader appends an entry into its log containing the key, the value, the request id, and the client's name. It then runs through the Raft consensus algorithm until the entry is committed. In this implementation, committing a \verb|SET| request entry on any node involves writing it to the replicated state, then messaging the client recorded in the entry to let it know that the request has been completed. This results in all nodes sending the same success message to the client, which improves the odds that the client will see one of them in the event of a network partition. However, it also means that the client has to deal with filtering out duplicates. It does this by using the unique request id, which is included in all messages sent from and to the client.

The cluster may fail to respond to \verb|SET| requests as with \verb|GET| requests. However, in the case of \verb|SET|, retrying may lead to duplicate entries being committed into the replicated state. This is an acceptable outcome -- the last \verb|SET| entry will effectively override any earlier ones, and since the last \verb|SET| entry must be committed before the client reports success back to the broker, we can simply choose the commit of the last \verb|SET| entry to be the linearization point of the request from the broker.
\subsection{Fault tolerance}
Due to the majority-based voting of the Raft internals, this implementation is able to handle the failure of a minority of its nodes while being able to return to a state where it available. With respect to consistency, the original paper by Ongaro and Ousterhout gives a list of consistency properties in Figure 3 which have been shown to hold at all times. However, this only implies that this implementation is only eventually consistent. Consider a run where the leader has committed some key (and therefore sent notification to the client). If the leader fails between now and when it next contacts its followers, none of its followers will have committed the entry, so when the next leader comes to power, it will not have committed the entry either. If the client issues a \verb|GET| request at this point, the leader may not have determined that the entry was safe to commit before responding to the \verb|GET|, so the client may receive stale data. However, as the algorithm runs, the entry will eventually be committed due to the Leader Completeness property, guaranteeing eventual consistency. Unfortunately, since election timeouts are randomized and the broker can't perform commands conditioned on the contents of messages, I can't come up with a script that demonstrates this happening.

With respect to availability, it's easy to see how Raft may become unavailable under particular circumstances. If luckily timed transient node failures keep hitting the leaders just after they are elected, Raft will be unavailable. In addition, dueling candidates during elections can also impact availability. However, this is a rare case, so in most circumstances with isolated single failures, availability will only be impacted for a fraction of a second as leader election takes place. Thus, if we view this implementation in the CAP framework, it sits somewhere in the middle. It guarantees eventual consistency (but probably fully consistent without the lucky timing described above), and if we view error messages as the equivalent of not completing in a finite time, it has availability in the absence of really luckily timed faults.
\section{Script descriptions}
First, before I start describing the scripts, there is a small caveat. Since the elections are randomized to some extent, there can be a pretty long setup period in the beginning where there are no leaders. If this happens and if the first thing in the broker script is a set, then everything later on goes awry as well. Thus, everything I'm describing here assumes speedy elections, which happen most of the time but not always.
\subsection{Leader failure}
This script is named \verb|leader_failure.chi|, and it illustrates the scenario where the leader experiences a temporary network partition that isolates it from the rest of the network. The desired action in this case is for the four remaining followers to elect a new leader and keep serving requests, and upon rejoining, the former leader will realize that it is out-of-date, revert to a follower, and catch up on its log.

The logging output of a sample run is attached as \verb|leader_failure.out|. At line 76, the client first receives a \verb|SET| request from the broker and forwards it to \verb|A1|, who happens to be the leader. It then broadcasts AppendEntries messages, and \verb|A2| and \verb|A3| seem to have gotten them first (lines 80, 84). Having received affirmation from a majority, \verb|A1| goes ahead and commits (line 129, the ordering is kinda messed up), and the broker receives a response (127). Next, the broker issues a \verb|GET|, which \verb|A1| responds to immediately after the next heartbeat (129, 136). The broker then creates a partition, and $151$ms later, \verb|A3| times out. In the subsequent lines, \verb|A2| and \verb|A4| grant votes to \verb|A3|, while \verb|A5| fails to grant a vote due to an incompatibility in its log (probably from starting so late). In any case, \verb|A3| receives a majority and becomes leader (165). Meanwhile, the client has timed out contacting \verb|A1| for the \verb|GET|, as \verb|A1| is no longer able to receive heartbeat responses (195). Thus, it picks a random node to try (\verb|A2|) and gets redirected to \verb|A3| (200). \verb|A3| is then able to respond to the \verb|GET| request as leader, and similarly with the subsequent \verb|SET|. 

Later, at line 235, the network partition is resolved, and \verb|A1| is able to contact other nodes again. Immediately, it sends out a heartbeat and is rejected by \verb|A5| because its term is lagging (237). \verb|A1| then receives AppendEntries from the current leader \verb|A3|, rejects them a few times because it is too out-of-date, and finally succeeds in appending 3 entries (241, 249, 253). Meanwhile, \verb|A3| handles the \verb|GET| request from the client in routine fashion, without consulting with the out-of-date \verb|A1|.

This script demonstrates the failstop tolerance of the data store, as well as its ability to handle rejoining nodes gracefully. Since a network partition of one node is equivalent to the failstop of that node from the perspective of the rest of the system, it produces the same effect as halting the other node and starting it again.
\subsection{Partitions}
This script is named \verb|leader_partition.chi|, and it runs the scenario where the data store gets partitioned into a group of 3 and a group of 2, where the leader ends up in the partition with two nodes. The desired action by the cluster is for the group of three nodes to recognize that they form a majority, elect a leader among them, and keep going, while the group with two nodes remains ineffective. After the partition is resolved, the two nodes in the smaller group should recognize that they are out-of-date and revert to being followers while updating themselves.

The logging output of a sample run is attached as \verb|partition.out|. The beginning goes much as the previous script, with \verb|A1| being elected leader and processing a \verb|SET| and a \verb|GET| request. At line 105, the network partition is created, and $180$ms later, \verb|A2| times out and begins elections in the group of three. It wins the election at line 125 and becomes the leader of the group of three. Meanwhile, the client has submitted the next \verb|GET| request to \verb|A1| (101), but since it is unable to contact a majority to verify that it is still leader, it is unable to respond. Thus, the client times out, tries to contact \verb|A5| next, and is redirected to \verb|A2| (139, 144). \verb|A2| responds to this immediately (149), and the client proceeds with a \verb|SET| of \verb|k1| to $30$, which runs normally and succeeds (151, 174). 

Finally, the partition is resolved at line 175, and the client simultaneously issues a \verb|SET| (177). Next, we see \verb|A1| and \verb|A3| realize that they are behind on terms (181, 185, 189) and revert to being followers of \verb|A2|. At the same time, \verb|A2| handles the \verb|SET| request from the client and distributes the new value of \verb|k2| to all followers, including \verb|A1| and \verb|A3|, and they both catch up at log index $4$. The \verb|SET| proceeds, and after \verb|A1| and \verb|A3| are caught up, the cluster reverts to normal operation.

This script demonstrates the partition tolerance of the data store. More specifically, it retains consistency while experiencing a network partition, while suffering brief loss of availability during the leader changeover (with potentially unbounded duration of the changeover process).
\section{Issues and lessons learned}
\subsection{Implementation}
The biggest issue that I had here was figuring out exactly how the event-driven model of the Tornado IOLoop works and how to integrate timeouts with it. Most networked programs I've written before were threaded and operated on the socket level, which means that I had control over how timeouts and message-driven events interact. The main concern I had was race conditions caused by timeouts triggering in the middle of a function execution. Fortunately, as a read over the Tornado source code revealed, the triggered timeouts were simply placed on a queue for later execution, so it turned out to be a non-issue.

In addition, the design of the broker did not allow for very effective debugging. Since all the output got redirected to \verb|/dev/null|, I was unable to track down Python exceptions that got thrown when I messed up with indexing. I eventually resolved this problem by telling the interpreter to use a log file in \verb|/tmp| as stdout and stderr in the code for initializing the nodes. Beyond these two issues, the implementation process was fairly straightforward. Python and ZMQ make it very easy to abstract away the gritty details of a network, and that reduced the workload considerably compared to if I were doing this in C.
\subsection{Design}
The main issue in design was the limited ability of the broker to accept and send requests to arbitrary nodes. The design of Raft calls for a centralized leader, but who this leader is is completely arbitrary and left up to chance. I was able to essentially force a node to be the leader by starting it first then making it time out very early, but if I wanted that node to fail, I'd have no idea who the next leader was, so I'd be unable to hard-code that into the broker. The solution, of course, was to write a client node to sit between the broker and the cluster to handle all the redirects, and this seems to replicate real applications in that an extra layer of abstraction between the actual user and your server can be a very useful thing.

Initially, when I was considering ideas for this project, I decided upon a Scatter-style\footnote{Glendenning et al., \href{http://homes.cs.washington.edu/~arvind/papers/scatter.pdf}{Scalable consistency in Scatter}. 2011} combination of Raft and Chord, where the DHT overlay would use the Chord topology, and each ``node'' in the DHT would be a Raft cluster of 5 nodes. This turned out to be unfeasible due to time constraints, but I did learn a few things from thinking about how to integrate the two. First, Scatter's approach to getting the Raft/Paxos clusters to agree turns out to be widely applicable -- with the Chord overlay, the only modification needed is to use 2PC to make the changes consistent with neighboring clusters, then broadcast the change to everyone in the finger table so it can propagate around the ring faster. In addition, the issue of making sure the changes maintain consistency in the face of failures is simplified by the fact that replication is taken care of within the Raft/Paxos clusters, so different DHT nodes with copies of the same key won't need to worry about using the DHT protocols to keep themselves consistent. Given what I've implemented already, this probably won't be too much additional work -- making a new subclass of \verb|RaftBaseNode| with the rather simple Chord DHT protocols built in would be sufficient.
\end{document}

