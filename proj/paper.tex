\documentclass[psamsfonts]{amsart}

\usepackage{amssymb,amsfonts}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{hyperref}

\makeatletter
\makeatother

\bibliographystyle{plain}
\title{A distributed key-value store using Raft}

\author{Hall Liu}

\begin{document}

\maketitle
\section{Summary of the protocol}
This implementation of a distributed, fault-tolerant key-value store is based on the Raft algorithm by Ongaro and Ousterhout \footnote{Diego Ongaro, John Ousterhout; \href{https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf}{In search of an Understandable Consensus Algorithm}. 2014}. The Raft algorithm is an algorithm which implements a fault-tolerant replicated state machine, where in this case the state to be replicated is the dictionary containing the keys and values that are to be stored. In this section, I will give a brief summary of the fundamentals of the algorithm.

Each node participating in a Raft cluster has some internal state which is distinct from the state of the key-value store. The former will be referred to as the \emph{Raft state}, and the latter will be referred to as the \emph{replicated state}. 
\subsection{Roles of the nodes}
Each node in a Raft cluster may take one of three roles: {\it leader}, {\it follower}, or {\it candidate}. The leader is the node responsible for coordinating communication between the nodes and client. In normal operation, the leader receives all messages coming from outside the cluster, and it is the recipient of all messages originating with other nodes in the cluster. All other nodes are followers, which keep a copy of the leader's replicated state and update it in accordance with the leader's instructions. The candidate state is invoked when communication with the leader is disrupted for some reason -- it is the transitional state that a follower goes into in order to become the new leader of the cluster.
\subsection{Terms and Leader election}
The leader of a Raft cluster is elected for the duration of a {\it term}, which is a part of the Raft state that the algorithm aims to keep consistent across all nodes. The term is a monotonically increasing variable which is included in each message that is passed from one node to another. Upon receiving any message, a node will verify that its term matches the term of the sender. If its own term is lower, it will update its term to the term of the sender, fall back to a follower state, and wait for instructions from any emerging leaders. If its own term is higher, the message will typically be disregarded, except in the case where the leader receives an outdated reply from a follower.

During normal operation, in the absence of faults or significant delays, a term will last indefinitely. However, in the event of a fault, each follower has an internal timeout which triggers after $t$ seconds without contact from the leader node, where $t$ is drawn randomly between $150$ms and $300$ms. Upon reaching this timeout, a follower will increment its term number and send a vote request to all its peers. All nodes carry who they voted for in any term, and it will grant a vote upon request if the requester is at least up-to-date as it is. Then, once a candidate receives votes from a majority of its peers (including itself -- a candidate immediately casts a vote for itself upon becoming a candidate), it will become a leader which has a log at least as up-to-date as a majority of the cluster.
\subsection{The Raft log and AppendEntries messages}
Once elected into office, the leader communicates with all other nodes in the cluster through \emph{AppendEntries} messages (these are RPCs in the original paper, but this implementation uses message-passing for communication). These messages are the heart of the algorithm -- they carry information about the leader's replicated state and also quell the followers' instinct to rebel. 

The leader is the recipient of all requests from clients to modify the replicated state of the cluster. To that purpose, it encodes all such requests as a sequence of commands that may be applied to the replicated state, and stores these commands in sequence in the \emph{Raft log}. The Raft log is the mechanism by which followers keep their replicated state consistent -- if two logs agree, then the state obtained by applying the logs to the replicated state must also agree.

AppendEntries messages, then, are requests from the leader to followers to append entries to their copy of the Raft log. In order for the log to be kept consistent, each entry in the log is recorded with the term during which it was generated by the leader. Each AppendEntries message contains the index and the term of the entry immediately prior (in the leader's log) to the first entry enclosed with the message. If the follower's log contains an entry at that index with that term, it will overwrite its own log past that point with the entries contained in the message. If not, it will respond with an error message.

AppendEntries also carry information about when to commit Raft log entries to a node's copy of the replicated state. Since committed changes are assumed to be irreversible, the leader will only instruct followers to commit an entry after it knows that a majority of the followers have the entry in their log in the proper index. This way, even if faults happen, any newly elected leader must also have this entry in their log, as otherwise they would not receive votes from a majority due to being out-of-date. The commit information is carried as an integer in each AppendEntries message -- the leader sends the index of the last entry that it has committed into its replicated state.

Finally, AppendEntries messages also serve to keep the leader in power. Whenever a node receives an AppendEntries message with a term at least as high as its own, it will revert back to follower status and reset its timeout. Thus, if a leader sends out AppendEntries messages (possibly with zero actual entries attached) with greater frequency than once every $150$ms, no other nodes will attempt to become leader.
\section{Description of the implementation}
\subsection{Structure and requests}
The actual implementation is done in Python building on the provided example node as a base. There are two distinct components to this implementation -- the client node and the server node. The server node code implements the Raft state and its transitions and stores the replicated state of the cluster. The client node serves as a middleman between the cluster and the broker due to the broker's limited abilities to address nodes. A typical run with the broker scripts will start the client node

When a 
\end{document}

