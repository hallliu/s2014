\documentclass[psamsfonts]{amsart}

\usepackage{amssymb,amsfonts}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{hyperref}

\makeatletter
\makeatother

\bibliographystyle{plain}
\title{A distributed key-value store using Raft}

\author{Hall Liu}

\begin{document}

\maketitle
\section{Summary of the protocol}
This implementation of a distributed, fault-tolerant key-value store is based on the Raft algorithm by Ongaro and Ousterhout \footnote{Diego Ongaro, John Ousterhout; \href{https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf}{In search of an Understandable Consensus Algorithm}. 2014}. The Raft algorithm is an algorithm which implements a fault-tolerant replicated state machine, where in this case the state to be replicated is the dictionary containing the keys and values that are to be stored. In this section, I will give a brief summary of the fundamentals of the algorithm.

Each node participating in a Raft cluster has some internal state which is distinct from the state of the key-value store. The former will be referred to as the \emph{Raft state}, and the latter will be referred to as the \emph{replicated state}. 
\subsection{Roles of the nodes}
Each node in a Raft cluster may take one of three roles: {\it leader}, {\it follower}, or {\it candidate}. The leader is the node responsible for coordinating communication between the nodes and client. In normal operation, the leader receives all messages coming from outside the cluster, and it is the recipient of all messages originating with other nodes in the cluster. All other nodes are followers, which keep a copy of the leader's replicated state and update it in accordance with the leader's instructions. The candidate state is invoked when communication with the leader is disrupted for some reason -- it is the transitional state that a follower goes into in order to become the new leader of the cluster.
\subsection{Terms and Leader election}
The leader of a Raft cluster is elected for the duration of a {\it term}, which is a part of the Raft state that the algorithm aims to keep consistent across all nodes. The term is a monotonically increasing variable which is included in each message that is passed from one node to another. Upon receiving any message, a node will verify that its term matches the term of the sender. If its own term is lower, it will update its term to the term of the sender, fall back to a follower state, and wait for instructions from any emerging leaders. If its own term is higher, the message will typically be disregarded, except in the case where the leader receives an outdated reply from a follower.

During normal operation, in the absence of faults or significant delays, a term will last indefinitely. However, in the event of a fault, each follower has an internal timeout which triggers after $t$ seconds without contact from the leader node, where $t$ is drawn randomly between $150$ms and $300$ms. Upon reaching this timeout, a follower will increment its term number and send a vote request to all its peers. All nodes carry who they voted for in any term, and it will grant a vote upon request if the requester is at least up-to-date as it is. Then, once a candidate receives votes from a majority of its peers (including itself -- a candidate immediately casts a vote for itself upon becoming a candidate), it will become a leader which has a log at least as up-to-date as a majority of the cluster.
\subsection{The Raft log and AppendEntries messages}
Once elected into office, the leader communicates with all other nodes in the cluster through \emph{AppendEntries} messages (these are RPCs in the original paper, but this implementation uses message-passing for communication). These messages are the heart of the algorithm -- they carry information about the leader's replicated state and also quell the followers' instinct to rebel. 

The leader is the recipient of all requests from clients to modify the replicated state of the cluster. To that purpose, it encodes all such requests as a sequence of commands that may be applied to the replicated state, and stores these commands in sequence in the \emph{Raft log}. The Raft log is the mechanism by which followers keep their replicated state consistent -- if two logs agree, then the state obtained by applying the logs to the replicated state must also agree.

AppendEntries messages, then, are requests from the leader to followers to append entries to their copy of the Raft log. In order for the log to be kept consistent, each entry in the log is recorded with the term during which it was generated by the leader. Each AppendEntries message contains the index and the term of the entry immediately prior (in the leader's log) to the first entry enclosed with the message. If the follower's log contains an entry at that index with that term, it will overwrite its own log past that point with the entries contained in the message. If not, it will respond with an error message.

AppendEntries also carry information about when to commit Raft log entries to a node's copy of the replicated state. Since committed changes are assumed to be irreversible, the leader will only instruct followers to commit an entry after it knows that a majority of the followers have the entry in their log in the proper index. This way, even if faults happen, any newly elected leader must also have this entry in their log, as otherwise they would not receive votes from a majority due to being out-of-date. The commit information is carried as an integer in each AppendEntries message -- the leader sends the index of the last entry that it has committed into its replicated state.

Finally, AppendEntries messages also serve to keep the leader in power. Whenever a node receives an AppendEntries message with a term at least as high as its own, it will revert back to follower status and reset its timeout. Thus, if a leader sends out AppendEntries messages (possibly with zero actual entries attached) with greater frequency than once every $150$ms, no other nodes will attempt to become leader.
\section{Description of the implementation}
\subsection{Structure and requests}
The actual implementation is done in Python building on the provided example node as a base. There are two distinct components to this implementation -- the client node and the server node. The server node code implements the Raft state and its transitions and stores the replicated state of the cluster. The client node serves as a middleman between the cluster and the broker due to the broker's limited abilities to address nodes. A typical run with the broker scripts will start the client nodes first, then the server nodes.

\subsection{GET requests}
When the broker issues a get request, it should always be directed towards one of the clients. Upon receiving this request, the client node associates the info in the request to the id provided by the broker so that it can formulate a response back to the broker after the Raft cluster responds. It then generates a new id which is unique with respect to all other clients and requests by prepending its name to a counter incremented every time an id is generated. The client node will then associate this new id with the broker's id for the request, then send off a \verb|GET| message to who it thinks the Raft leader is.

The client node initially assumes that the first entry in its list of Raft nodes is the leader. However, if it sends a \verb|GET| message to a node which is not the leader, that node will reply with a \verb|redirect| message providing who it thinks that the current leader is. The client will then record this as the new leader and retry the request. If the cluster is currently transitioning between leaders, the response to the \verb|redirect| may be null. In this case, the client retries with a randomly chosen node.

Due to the nature of log replication, the leader will always have the most up-to-date replicated state if it is still the sole leader and has not been deposed. Thus, there is no need for the leader to append an entry into its log -- it can just respond using its copy of the replicated state after verifying that it has not been deposed. To do this, after receiving the \verb|GET| request from the client, the cluster leader sends out a possibly empty AppendEntries message tagged with the client's request id to all followers in order to solicit a response. It then associates the request id to client request, then exits to process other messages. The leader will keep track of the responses associated with that id, and once it receives a majority affirming that the term has not ended, it will reply back to the client with the appropriate data indicating success, including the client's request id that was sent with the initial request. When the client receives this message, it will retrieve the broker's request using the id in the reply, then formulate a reply back to the broker indicating success and carrying the value.

However, if a fault occurs during this process which leaves the leader unable to contact a majority of nodes, no response may be generated. To handle this case, the client nodes associate a $300$ms timeout with each request, and will retry the request if this timeout expires. The value of the timeout was chosen to be small while still being long enough to wait out leadership changes. After some number of retries (I picked $5$ to allow for two cycles of timeout-redirects), the client will return an error to the broker. 
\subsection{SET requests}
The beginning and end of a \verb|SET| request look much like those of \verb|GET| requests, wherein the broker submits a request to the client, the client repackages it for the cluster, and the response follows the same chain backwards. The middle part of a \verb|SET| request differs, as setting a value requires a change to be committed into the replicated state. To do this, the leader appends an entry into its log containing the key, the value, the request id, and the client's name. It then runs through the Raft consensus algorithm until the entry is committed. In this implementation, committing a \verb|SET| request entry on any node involves writing it to the replicated state, then messaging the client recorded in the entry to let it know that the request has been completed. This results in all nodes sending the same success message to the client, which improves the odds that the client will see one of them in the event of a network partition. However, it also means that the client has to deal with filtering out duplicates. It does this by using the unique request id, which is included in all messages sent from and to the client.
\end{document}

